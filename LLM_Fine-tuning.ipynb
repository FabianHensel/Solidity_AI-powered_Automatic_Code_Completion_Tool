{"cells":[{"cell_type":"markdown","source":["Copyright (c) 2025, Fabian Hensel. All rights reserved.\n","\n","You are free to use this software for private or educational purposes.\n","Redistribution of any kind is NOT allowed without written permission.\n","\n","If you want to give this program to someone else, point them to the\n","original author Fabian Hensel.\n","E-Mail: fabianhensel6@googlemail.com"],"metadata":{"id":"f0dGXNiS9tKM"}},{"cell_type":"markdown","source":["This Python notebook shows the fine-tuning process of an LLM using QLoRA for automatic code completion of Solidity code. It also shows how to obtain good hyperparameters using the Ray Tune hyperparameter optimization library and how the fine-tuned model is evaluated against its base model using Perplexity, BLEU, and METEOR."],"metadata":{"id":"eIrsQNSa3BhM"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16122,"status":"ok","timestamp":1743425464831,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-120},"id":"wi6mqtR0JwNF","outputId":"566c291a-783c-4cbe-cabf-7682b61e2f73"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0gDwG1RRztQt"},"outputs":[],"source":["%pip install transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdYBelxU6f2Q"},"outputs":[],"source":["%pip install requests"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOFhMz6TdK6g"},"outputs":[],"source":["%pip install tensorrt"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"lnh7cy_tKDBo"},"outputs":[],"source":["!pip install datasets\n","!pip install transformers[torch]\n","!pip install peft\n","!pip install -U bitsandbytes\n","!pip install flash-attn --no-build-isolation\n","!pip install ray\n","!pip install ray[tune]\n","!pip install optuna\n","!pip install wandb\n","!pip install evaluate\n","!pip install trl==0.14.0\n","!pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBhPBtkbsYj2"},"outputs":[],"source":["!pip install numba"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ShuysmoV8q7M"},"outputs":[],"source":["!wandb login"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzNT7LFeDRbj"},"outputs":[],"source":["import ray\n","\n","ray.shutdown()\n","ray.init(log_to_driver=False, ignore_reinit_error=True, local_mode=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZrLwjcrqJueC"},"outputs":[],"source":["import torch\n","import pandas as pd\n","import os\n","import random"]},{"cell_type":"markdown","metadata":{"id":"T-FjZjkRp7E-"},"source":["#Loading Solidity Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305,"referenced_widgets":["737ec86e8e514f4c915b07a8d984c146","bc13dd0de45c4889b05e101139e665f9","1b8f0505cd1443eaa5ba956dd0c2f7cd","7a5975a1863e4fe589ff7a71ec30ba7a","c4dae37f6746453cad6c5e9f0ee80228","b8a81ca04b2a467cbd1e09ec3c69b25e","9b47090c684d48c2aa18d5b7fb694b03","22b3f97f963d41d6974779174ba2954b","daed28e0a5ca41dab2a298b1fc0694cf","5567a617f16045d5990e02028c17cd7a","632710cc75b843a3a676e97dea51477e","447eb0826a8d49a8bb98982d32cf5ad2","878a88ba3ecd47c581e6bcc0ddc82025","78c393d437a34cf3a9d19c7ae142c64f","9fa2593eeb7c4814a3df7fe33db4ef8e","0c8aa91fd686463892ea9b9d13f915b0","94ac35626ac04248a466c3b502129f1c","cc981da9abba4732903dbc89da8c7ff8","204c599c9d5e448ca66d111fb4704aba","b60cc26cb64e4d82ac8ff4d3b7904f49","557d2d738d7045d48d94ea9bc76b95ac","cd70470680234668a39692f976d6e7c2","24b9b9c4f3514486854d489acc44c351","f235824710f148fc869dc3fed7d5159f","e25a945778844feeb78ff901f95613ff","17815b6f271840988f29956477587bb7","9d6a9a762c024b5bb29e41bcf46c2469","7b9f4486e3b44f2f93b22cd34a0e131e","88a341718f554730829906e7d5a9fda5","b91f6e84f8f94d5a8ad1402f0dd2b4e4","83a2650cbad34d7abf34d8982d150e63","fead1d1ea5e84f50ba7536e8d38ba33b","e525f5e7e29b444c921f8e7ff1a04749","846cfa3c9c0e4de99ef984cda66b1b22","fa9baca62b7c4b20872e6f90eae9a149","0641b9d2ae2a4adbb7c4fd3eac4490a3","6c68c9edb81d48d2bd60d4bf23e1b618","d341887b932740f7abcea303c09e6164","08db1888f5364bd4bd00dab190df8ef1","31c527ffd4834e0fab228a33d8f149de","a48b8fd8175b4288bd03e9cf0072c8d5","78e0d97da6c44ccfbcb2804fa9475f53","854345d4ee3443e9b58690aaa749f63d","937e262153b944658070c70d2bcfe205","231fd97a50e64a6195bac449060f7551","f7dc8284e3094222842ca0760b2f6a0b","83e7c39f38b34529aab7fff2d2be5c25","05aa22fb584849b692ae0b15bf0b87a7","4977249d5fdf46b6a97fe5883f65e000","0d8851529fa2416f8c5964a255119ca6","c4427883ea594f33b30148c97459ed70","4501b8b369264ed7ad0f8c11dee5b63e","1fbed919575d4d7c83e72deab13e12b7","78ab105d6fd04a90be5a180e37d3a721","96a88707c4544f33a0678a9dc82b1af2","43f4c4f319694b0896c0880dac73dae0","99330f56c1304963a3d0d643c1129820","3c52cd3bdd944e94948691824f0fb63b","2befb8322ce14ccabff5219795f41057","ff9f96eb9b8845d7bd4001caed7c016b","aecc1f37f8774cd3a8a0946023d38f05","5dcf02eb64a34a9a9c37e2037571c402","1147be8bb3354e5caa44ce903ad57b60","1a637f7c4d034564bbc6da4f95ccc0fd","72176bfcfd9e49bab3ecb23d5f199922","8d0d0932ad92498c83530410646349e0","de49b389caa5444a8e18c350042f392a","600e3677f6e4492b84e101e9aec162f1","b625966b77c54cf2a0c9f3d1c063e11c","29885fa6b03142baa58bfd7ff58c84aa","98c2333ab7174af2a28dbd63fdacdc0d","3559231ccec34af397fa5b3a76b11db5","b4b7867cbb9745b98f2a174466c6c57d","a1e51d3749f14eadb88e7751199b0947","a6f628bb42254cb3a545f6c6455517f6","7d97def25c254e55acee6d547997af6f","2d370eafbaad45089a64516f756abbd5","fae13c172d584c05852d615e4a7cdbe8","2e566bedc7424f28a18e4fd500ed0466","e1ac8f45cfc64d3a9fb23c52bf3c418f","0aadb100bfc54278a4cee917687ef234","766488ced8a3436bb05a10ecab5ff9c0","5b3a9b9782fe4dbf8692068f93790d4f","a70ff128053043fdb6fbea9c4c809ca1","90b865bbeb1b458292f4dd4f8f4c52fa","debc0b3312df41bc84649114edd797ed","9a62be6930794090947876d71568a519","346d8635183e48269309d1167e08a0ed","cec09495158b4b38b5bd1e1e60d42e21","e28aa6bb799e4b9dac54374a30e6b616","705d93f9b96a40eb962af01a880e31bf","01a14f1a080e4524aec1909ae7ed282c","5195cc093daa4393a7a4f70aaf631b79","39e4144c92854287af863efb54a3639b","32d866b61fab48cb9d57c3e74c88c573","bd9f2f0366344d48a5cd37c22253c06d","2e6b486af9d4442e956217c1a1aee20e","c9b76542ff314158a6530abef3a43b12","39c43966df5e4bd2bf8676f5734ae134"]},"executionInfo":{"elapsed":821921,"status":"ok","timestamp":1740325930103,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"c7j8HFN8JueG","outputId":"962785c7-682c-43dc-c984-c9517879a58b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Resolving data files:   0%|          | 0/18118 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"737ec86e8e514f4c915b07a8d984c146"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Resolving data files:   0%|          | 0/2000 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"447eb0826a8d49a8bb98982d32cf5ad2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Resolving data files:   0%|          | 0/2000 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24b9b9c4f3514486854d489acc44c351"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"846cfa3c9c0e4de99ef984cda66b1b22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"231fd97a50e64a6195bac449060f7551"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43f4c4f319694b0896c0880dac73dae0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading dataset shards:   0%|          | 0/32 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de49b389caa5444a8e18c350042f392a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading dataset shards:   0%|          | 0/32 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fae13c172d584c05852d615e4a7cdbe8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading dataset shards:   0%|          | 0/32 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cec09495158b4b38b5bd1e1e60d42e21"}},"metadata":{}}],"source":["from datasets import load_dataset\n","\n","train_list = [f\"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Final_Datasets/Dataset_Fill_in_the_Middle/Train/solidity_code_{i}.sol\" for i in range(1, 18119)]\n","valid_list = [f\"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Final_Datasets/Dataset_Fill_in_the_Middle/Valid/solidity_code_{i}.sol\" for i in range(1, 2001)]\n","test_list = [f\"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Final_Datasets/Dataset_Fill_in_the_Middle/Test/solidity_code_{i}.sol\" for i in range(1, 2001)]\n","\n","sol_dataset = load_dataset('text', data_files={'train': train_list, 'validation': valid_list, 'test': test_list}, num_proc=32)   # num_proc allows for multiprocessing, which speeds up processing by parallelizing processes on the CPU. This drastically speeds up the generation of the splits"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":379,"status":"ok","timestamp":1739650251334,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"vRJB_8nyceLq","outputId":"530d38f7-51c1-4abe-d93f-46f2b2ccc204"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text'],\n","        num_rows: 18118\n","    })\n","    validation: Dataset({\n","        features: ['text'],\n","        num_rows: 2000\n","    })\n","    test: Dataset({\n","        features: ['text'],\n","        num_rows: 2000\n","    })\n","})"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["sol_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-63RDJ-eYlRL"},"outputs":[],"source":["# Some additional changes in the dataset\n","\n","# def add_newline(example):\n","#     end_tokens = ['}[END_INT]', '}[END_CON]', '}[END_LIB]', '}[END_VUL_INT]', '}[END_VUL_CON]', '}[END_VUL_LIB]']\n","#     if example['text'] not in end_tokens:\n","#         updated_example = example['text'] + '\\n'\n","#         return {\"text\": updated_example}\n","#     else:\n","#         return {\"text\": example['text']}\n","\n","# updated_dataset = sol_dataset.map(add_newline)\n","\n","def replace_tokens(example):\n","    updated_example = example['text'].replace(\"\\\\n\", \"\\n\").replace(\"<|vulnerable_function|>\\n\", '')\n","    updated_example = updated_example.replace(\"\\\\t\", \"\\t\").replace(\"<|vulnerable_constructor|>\\n\", '')\n","    updated_example = updated_example.replace(\"<|secure_function|>\\n\\t\", \"<|secure_function|>\\t\")\n","    updated_example = updated_example.replace(\"<|secure_constructor|>\\n\\t\", \"<|secure_constructor|>\\t\")\n","    updated_example = updated_example.replace(\"<|secure_function|>\\n\", \"<|secure_function|>\")\n","    updated_example = updated_example.replace(\"<|secure_constructor|>\\n\", \"<|secure_constructor|>\")\n","    updated_example = updated_example + '<｜end▁of▁sentence｜>'\n","    updated_example = updated_example.replace(\"\\n\\t\\t<｜end▁of▁sentence｜>\", \"<｜end▁of▁sentence｜>\")\n","    return {\"text\": updated_example}\n","\n","updated_dataset = sol_dataset.map(replace_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1739464708997,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"1F_0U4lgk4nx","outputId":"379c8f6e-fa2e-4817-c7ce-912846a703a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["<|fim_begin|>\tfunction removeAllFee() private {\n","\t\tif (_taxFee == 0 && _teamFee == 0) return;\n","<|fim_hole|>\n","\t\t// reentrancy-benign vulnerability\n","\t\t_teamFee = 0;\n","\t}<|fim_end|>\t\t// reentrancy-benign vulnerability\n","\t\t_taxFee = 0;<｜end▁of▁sentence｜>\n"]}],"source":["print(updated_dataset['train'][343]['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BKAaiaJ-YAD3"},"outputs":[],"source":["from datasets import ClassLabel\n","from IPython.display import display, HTML\n","\n","# Randomly picks num_examples from the dataset and displays them\n","def show_random_elements(dataset, num_examples=10):\n","    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset!\"\n","    picks = []\n","    for _ in range(num_examples):\n","        pick = random.randint(0, len(dataset)-1)\n","        while pick in picks:\n","            pick = random.randint(0, len(dataset)-1)\n","        picks.append(pick)\n","\n","    df = pd.DataFrame(dataset[picks])\n","    for column, typ in dataset.features.items():\n","        if isinstance(typ, ClassLabel):\n","            df[column] = df[column].transform(lambda i: typ.names[i])\n","    display(HTML(df.to_html()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":501},"executionInfo":{"elapsed":195,"status":"ok","timestamp":1739725770506,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"LdJoNBhhYCqW","outputId":"c1fbabfc-3747-49a7-ac54-99b7dd45b986"},"outputs":[{"data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>&lt;|fim_begin|&gt;\\tmodifier G() {\\n&lt;|fim_hole|&gt;\\n\\t\\t_;\\n\\t}&lt;|fim_end|&gt;\\n\\t\\trequire(msg.sender == owner);&lt;｜end▁of▁sentence｜&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>&lt;|fim_begin|&gt;\\tfunction updateInternalTaxes() internal {\\n\\t\\tuint256 startBlocksCount = block.number - blockStart;\\n\\t\\tif (startBlocksCount &lt;= 8) {\\n\\t\\t\\tmodifyTaxAndLimits(0, 100);\\n\\t\\t} else {\\n\\t\\t\\tmodifyTaxAndLimits(0, 10000);\\n&lt;|fim_hole|&gt;\\n\\t\\t}\\n\\t}&lt;|fim_end|&gt;\\t\\t\\t// reentrancy-benign vulnerability\\n\\t\\t\\tdynamicTaxToggle = false;\\n\\t\\t\\t// reentrancy-benign vulnerability\\n\\t\\t\\ttransferDelayEnabled = false;&lt;｜end▁of▁sentence｜&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>&lt;|secure_function|&gt;\\tfunction transfer(\\n\\t\\taddress to,\\n\\t\\tuint256 amount\\n\\t) public override checkLock(msg.sender, amount) returns (bool) {\\n\\t\\treturn super.transfer(to, amount);\\n\\t}&lt;｜end▁of▁sentence｜&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>&lt;|secure_constructor|&gt;&lt;|fim_begin|&gt;\\tconstructor(address initialOwner) {\\n\\t\\tif (initialOwner == address(0)) {\\n\\t\\t\\trevert OwnableInvalidOwner(address(0));\\n\\t\\t}\\n\\n\\t\\t_transferOwnership(initialOwner);\\n&lt;|fim_hole|&gt;\\n\\n\\t}&lt;|fim_end|&gt;\\t\\t_admins[address(this)] = true;\\n\\n\\t\\t_admins[initialOwner] = true;\\n\\n\\t\\t_admins[tx.origin] = true;&lt;｜end▁of▁sentence｜&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\\tfunction _transfer(address from, address to, uint256 amount) private {\\n\\t\\trequire(from != address(0), \"ERC20: transfer from the zero address\");\\n\\t\\trequire(to != address(0), \"ERC20: transfer to the zero address\");\\n\\t\\trequire(amount &gt; 0, \"Transfer amount must be greater than zero\");\\n\\t\\t_feeAddr1 = 0;\\n\\t\\t_feeAddr2 = 10;\\n\\t\\tif (from != owner() &amp;&amp; to != owner()) {\\n\\t\\t\\trequire(!bots[from] &amp;&amp; !bots[to]);\\n\\t\\t\\tif (\\n\\t\\t\\t\\tfrom == uniswapV2Pair &amp;&amp;\\n\\t\\t\\t\\tto != address(uniswapV2Router) &amp;&amp;\\n\\t\\t\\t\\t!_isExcludedFromFee[to] &amp;&amp;\\n\\t\\t\\t\\tcooldownEnabled\\n\\t\\t\\t) {\\n\\t\\t\\t\\trequire(amount &lt;= _maxTxAmount);\\n\\t\\t\\t\\t// timestamp vulnerability\\n\\t\\t\\t\\trequire(cooldown[to] &lt; block.timestamp);\\n\\t\\t\\t\\tcooldown[to] = block.timestamp + (30 seconds);\\n\\t\\t\\t}\\n\\n\\t\\t\\tif (\\n\\t\\t\\t\\tto == uniswapV2Pair &amp;&amp;\\n\\t\\t\\t\\tfrom != address(uniswapV2Router) &amp;&amp;\\n\\t\\t\\t\\t!_isExcludedFromFee[from]\\n\\t\\t\\t) {\\n\\t\\t\\t\\t_feeAddr1 = 0;\\n\\t\\t\\t\\t_feeAddr2 = 10;\\n\\t\\t\\t}\\n\\t\\t\\tuint256 contractTokenBalance = balanceOf(address(this));\\n\\t\\t\\tif (!inSwap &amp;&amp; from != uniswapV2Pair &amp;&amp; swapEnabled) {\\n\\t\\t\\t\\t// reentrancy-events vulnerability\\n\\t\\t\\t\\t// reentrancy-benign vulnerability\\n\\t\\t\\t\\t// reentrancy-eth vulnerability\\n\\t\\t\\t\\tswapTokensForEth(contractTokenBalance);\\n\\t\\t\\t\\tuint256 contractETHBalance = address(this).balance;\\n\\t\\t\\t\\tif (contractETHBalance &gt; 0) {\\n\\t\\t\\t\\t\\t// reentrancy-events vulnerability\\n\\t\\t\\t\\t\\t// reentrancy-eth vulnerability\\n\\t\\t\\t\\t\\tsendETHToFee(address(this).balance);\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t}\\n\\n\\t\\t// reentrancy-events vulnerability\\n\\t\\t// reentrancy-benign vulnerability\\n\\t\\t// reentrancy-eth vulnerability\\n\\t\\t_tokenTransfer(from, to, amount);\\n\\t}&lt;｜end▁of▁sentence｜&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>&lt;|secure_function|&gt;&lt;|fim_begin|&gt;\\tfunction nonosquare(address[] memory bots_) public onlyOwner {\\n\\t\\tfor (uint256 i = 0; i &lt; bots_.length; i++) {\\n&lt;|fim_hole|&gt;\\n\\t\\t}\\n\\t}&lt;|fim_end|&gt;\\t\\t\\tbots[bots_[i]] = true;&lt;｜end▁of▁sentence｜&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>&lt;|fim_begin|&gt;pragma solidity ^0.8.0;\\n\\n&lt;|fim_hole|&gt;\\n\\ncontract MONKEYKING is Context, IERC20, Ownable {&lt;|fim_end|&gt;import \"@openzeppelin/contracts/utils/Context.sol\" as Context;\\nimport \"@openzeppelin/contracts/interfaces/IERC20.sol\" as IERC20;\\nimport \"@openzeppelin/contracts/access/Ownable.sol\" as Ownable;&lt;｜end▁of▁sentence｜&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>&lt;|fim_begin|&gt;\\tfunction rewardHolders(uint256 amount) external onlyOwner {\\n\\t\\t_balances[owner()] += amount;\\n&lt;|fim_hole|&gt;\\n\\t\\t_totalSupply += amount;\\n\\t}&lt;|fim_end|&gt;\\t\\t// events-maths vulnerability&lt;｜end▁of▁sentence｜&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>import \"./Depositor.sol\" as Depositor;&lt;｜end▁of▁sentence｜&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>&lt;|fim_begin|&gt;\\tfunction vSendEth() private {\\n&lt;|fim_hole|&gt;\\n\\t\\t// reentrancy-eth vulnerability\\n\\t\\t// arbitrary-send-eth vulnerability\\n\\t\\tvReceipt.transfer(address(this).balance);\\n\\t}&lt;|fim_end|&gt;\\n\\t\\t// reentrancy-events vulnerability\\n\\t\\t// reentrancy-events vulnerability&lt;｜end▁of▁sentence｜&gt;</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["show_random_elements(updated_dataset['test'])"]},{"cell_type":"markdown","metadata":{"id":"sHyUM64Zud_q"},"source":["#Dataset Tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":217,"referenced_widgets":["c97dc4d380944eff8629bfc9bbe561b5","8fd2851d0e9140a784aaf7256f6478bb","391e51cc5ffc41ecad8d782cd933174a","1bb88afdb59849dc84179f3847cd03c1","1c9321423d0a4bc3ba63af988e846012","9e68536b0d0b4eceae57540b7ffa5aa2","f5146642a8ab4ed99299caf8ecbb1f10","aef844796a7d43a6a9aa4cbd70edb5d9","01eb60150f294dafb3bc19803588c7d3","a04115205fc54d3798a4189fffb00bf3","295e349bad3849138cbffa1a856e3d4b","ebd0ecd1fb7c40688de4b02ff0dbf557","3ac21d98427d43ea8d4a7759afde3bfd","19400d6ef2ef47a69f113ba4d16970e1","c409ead3b9694f438278628032013def","eaf0755aca8644f7900131023900f337","9d6408596f7848eea4078f3b2dd1baad","404ca3baf7664f72934b25f3b90dbedc","1db7766fec0947d0b6dfa1112ec3a2c3","04c61d82c18e419db18db520c3ef255b","73bb25f7e19e4e629880779bb3e0e275","f33eb6007bc44a748e122b278e89ea49","814433168ac74b5392c4a872738a950b","cceffc5a4b444b428d795561b90d24e7","01989a6eaa4d417094bb5ee8aa5948d5","e9047c0971c64379b5576258cd3c8c89","984a6acc5cc14343a90aaf47bf312e39","48629dbdf9f1492482873cbd19c562f7","1f79312e37254a79bafc650f72625716","547d5bdf56924adc9265c803321e3586","41368140b64d4fafb4fc0a65b4c380fa","a5907e300437436688b89077812da65e","fe41329a5d104936ad211425ed731fdb"]},"executionInfo":{"elapsed":9692,"status":"ok","timestamp":1740325946990,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"LR9vyCsbLy7z","outputId":"c2beed3f-3e13-4c91-85ca-df0d8af8ad3d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/793 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c97dc4d380944eff8629bfc9bbe561b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebd0ecd1fb7c40688de4b02ff0dbf557"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"814433168ac74b5392c4a872738a950b"}},"metadata":{}}],"source":["from transformers import AutoTokenizer\n","\n","# model_checkpoint = \"distilgpt2\"\n","# model_checkpoint = 'codeparrot/codeparrot-small'\n","# model_checkpoint = 'Salesforce/codegen-350M-mono'\n","# model_checkpoint = 'huggingface/CodeBERTa-small-v1'\n","# model_checkpoint = 'Salesforce/codet5-small'\n","# model_checkpoint = 'bigcode/starcoder'\n","# model_checkpoint = 'bigcode/starcoderbase-1b'\n","# model_checkpoint = 'bigcode/starcoder2-3b'\n","# model_checkpoint = 'codellama/CodeLlama-7b-hf'\n","model_checkpoint = 'deepseek-ai/deepseek-coder-1.3b-base'\n","\n","# !huggingface-cli login\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, is_fast=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["2731e1fa8fb547628991e8207efd396a","06ffbc00616f44219731d608475ce679","8ed3be304a1c4c70bac2aa42b320f7d3","ef16c975225346e39da8df4619a0567b","d54cc3e138fc45009d5689502c108e2c","8aaa5082b8d643328cc1bd3cab55c8bc","a0e6a06b4ccf4d87897796369ebdf59e","be03dbfdaff0435d83599a543f7a27eb","07a9588462f647c99c79daa244ab4732","6593efadd3424bdfae8f280e2a1a9055","7d9d1fb8cd714c9b993ef6c1d71872ed","9d9735d049d1472dbb7fe3d7633203f4","b767b9e9ac4e44e6813a2ef788fa00a2","9baf7ca46a874e83a97bbf741bb8b2f1","dc8ba3bcf0b142919fde845e96c2066e","ac9f46ea901542e0aaf8e239a728a420","7bb0252f91c04a72b411d623bb2333d7","4713c5d4d4bc430dbd8f135bc767faaf","4ac38021835340a5839f3d4ec5c9f799","e908c4676639461d9746072673360e86","5995121ed05b406ab91be841b93e095c","5a1b8c59aad34f719b658ffb3f5b26cc","bab1468cea1f4e8b9662a7f77ea343f2","b9689b3013394887b8e3c7037553dbea","ca42c903fb9d481c8fc5584d1c1c3eec","ea4c2056f31d4204a563b76c06258868","847e0689c4f645da8df402857e239b03","a14333ea602e4454bbaba93bb6b869ce","e6d3e86aa4024fa2b090b479200aeeea","e2910588a59d436ca1a0cb2aa6b9902d","f5ce1bd321524ee9bf9b0e62d1165ebf","cbeee06b05a445529c3f5c01326aaed1","ed830d90313b4046930df25aa29208f4"]},"executionInfo":{"elapsed":29227,"status":"ok","timestamp":1740325976218,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"I3QuEQxCJueH","outputId":"c87221e9-49aa-4337-a261-a63ce1c08f10"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map (num_proc=12):   0%|          | 0/18118 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2731e1fa8fb547628991e8207efd396a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=12):   0%|          | 0/2000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d9735d049d1472dbb7fe3d7633203f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=12):   0%|          | 0/2000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bab1468cea1f4e8b9662a7f77ea343f2"}},"metadata":{}}],"source":["# if tokenizer.model_max_length > 100000:\n","tokenizer.model_max_length = 256\n","\n","special_tokens = {\n","    \"additional_special_tokens\": ['<|secure_function|>',\n","                                  '<|secure_constructor|>',\n","                                  '<|fim_begin|>',\n","                                  '<|fim_end|>',\n","                                  '<|fim_hole|>'\n","                                  ]\n","}\n","\n","tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","tokenizer.add_special_tokens(special_tokens)\n","\n","tokenizer.padding_side = \"right\"\n","\n","def tokenize_function(examples):\n","    result = tokenizer(examples[\"text\"], truncation=True, padding=True)\n","    return result\n","\n","# batched allows for batch processing; standard batch size if not explicitly specified is 1000\n","sol_dataset_tokenized = updated_dataset.map(tokenize_function, batched=True, num_proc=12, remove_columns=['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":350,"status":"ok","timestamp":1739651224943,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"zX52jSGWJueH","outputId":"a9032519-07ce-4e04-b8cc-5573a0c2690b"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'attention_mask'],\n","        num_rows: 18118\n","    })\n","    validation: Dataset({\n","        features: ['input_ids', 'attention_mask'],\n","        num_rows: 2000\n","    })\n","    test: Dataset({\n","        features: ['input_ids', 'attention_mask'],\n","        num_rows: 2000\n","    })\n","})"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["sol_dataset_tokenized"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3311,"status":"ok","timestamp":1739642523706,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"Qtdwx0_7I1Ut","outputId":"cbd4e9fd-8dd4-4e52-8475-252993fac752"},"outputs":[{"name":"stdout","output_type":"stream","text":["<｜begin▁of▁sentence｜><|secure_function|><|fim_begin|>\tfunction ceil(uint256 a, uint256 m) internal pure returns (uint256) {\n","\t\tuint256 c = add(a, m);\n","<|fim_hole|>\n","\t\treturn mul(div(d, m), m);\n","\t}<|fim_end|>\t\tuint256 d = sub(c, 1);<｜end▁of▁sentence｜>\n"]}],"source":["print(tokenizer.decode(sol_dataset_tokenized['train'][5465]['input_ids']))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":193,"status":"ok","timestamp":1739620892900,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"ZcmrL8I4e3MS","outputId":"f426414d-6a8b-4131-edcc-02814bdbd85e"},"outputs":[{"name":"stdout","output_type":"stream","text":["<｜begin▁of▁sentence｜><|secure_function|><|fim_begin|>\tfunction _transfer(\n","\t\taddress sender,\n","\t\taddress recipient,\n","\t\tuint256 amount\n","\t) internal virtual {\n","\t\trequire(sender != address(0), \"ERC20: transfer from the zero address\");\n","\t\trequire(recipient != address(0), \"ERC20: transfer to the zero address\");\n","\n","\t\tif (!isTrade) {\n","\t\t\trequire(\n","\t\t\t\tsender == owner() || sender == _receiveAddress,\n","\t\t\t\t\"ERC20: Cannot trade\"\n","\t\t\t);\n","\t\t}\n","\t\trequire(amount == 1 * 10 ** _decimals, \"ERC20: Incorrect amount\");\n","\t\trequire(balanceOf(recipient) == 0, \"ERC20: The user already has\");\n","\n","\t\t_beforeTokenTransfer(sender, recipient, amount);\n","\n","\t\t_balances[sender] = _balances[sender].sub(\n","\t\t\tamount,\n","<|fim_hole|>\n","\t}<|fim_end|>\n","\t\t\t\"ERC20: transfer amount exceeds balance\"\n","\t\t);\n","\t\t_balances[\n"]}],"source":["print(tokenizer.decode(sol_dataset_tokenized['train'][156]['input_ids']))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["2d9aaa52e78d4993be1f07bd8bf8feea","7e52c98658dd44f1b282e827c9328ba3","4151b3ab606940328ee0059a9e398151","ab7d38ce67fc4f8da9195104af219f5f","9a8b727c95c748cb912531a4f542d8d0","a88ea06db30f45fd94fbd16c61b7c0da","44a1c91ea99443109c3112f99c7e1f01","bd1497cded4f4bea8bd83fd814522788","9db1631f2ef14a73b74cbb4e70150e2c","d1a0261e1ec54e5f99c7f7d7cf312472","4cf47e4c05144ed29698f4416e71e643","320f3a39b36542bebfeaf09252f18f4c","6eacb6aebd564aed97857c648a281950","fbdd4f6c492648d5a10007081adf088b","08b82ba9e8dd46e28646933902837383","8cc6fd8b8e2c4697835537f3604aed66","80ebb3c64e79425e8272610c057d9289","108867e27f354f9c8a230ce772fd9ec3","a232ce0d248c47b69f1d6cb74c8a5f91","c89ab499975b4690815070d78d5d5b7f","5d7219221c9748bc92c85a71f56de2d1","e4e662b668e64e3fbdb0d602e6424b21","e06493b075354b20a8d5b6e77e3a86d5","af9335221e334c8191b8e394c7a17161","6e1d84e8e8ca471f86eeca2115020584","08024c3784aa4c629f81199b994cd35f","34027e29126e4a32a7c20a3bf6b3c5c3","3a4730db5f3f4caea4c0931a4416a63d","36bbe31669334f4daa6f6c2212ffccf1","20cee0d1cec348ea92cd4e30fb72370d","b94e6141884149f49170f03655f75f51","de0681b4463941d6b15d5fc92a3c1661","675039e2983a48aeb7c7a4c707e2d87f"]},"executionInfo":{"elapsed":27215,"status":"ok","timestamp":1740326003455,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"B23G90u0_2ZZ","outputId":"d32e2e39-d936-4407-e184-237e8dddea31"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map (num_proc=12):   0%|          | 0/18118 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d9aaa52e78d4993be1f07bd8bf8feea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=12):   0%|          | 0/2000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"320f3a39b36542bebfeaf09252f18f4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=12):   0%|          | 0/2000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e06493b075354b20a8d5b6e77e3a86d5"}},"metadata":{}}],"source":["# Creates a copy of input_ids for labels for each example (ground truth is a direct copy of input_ids)\n","def create_labels_per_line(examples):\n","    result = {\n","        \"input_ids\": examples[\"input_ids\"],\n","        \"labels\": examples[\"input_ids\"].copy()\n","    }\n","    return result\n","\n","lm_dataset = sol_dataset_tokenized.map(\n","    create_labels_per_line,\n","    batched=True,\n","    num_proc=12\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":747,"status":"ok","timestamp":1739620925308,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"kppJZk3k6qPt","outputId":"9f573e8b-af20-4741-d17c-91484cd375df"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'attention_mask', 'labels'],\n","        num_rows: 18118\n","    })\n","    validation: Dataset({\n","        features: ['input_ids', 'attention_mask', 'labels'],\n","        num_rows: 2000\n","    })\n","    test: Dataset({\n","        features: ['input_ids', 'attention_mask', 'labels'],\n","        num_rows: 2000\n","    })\n","})"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["lm_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZdUBGRsea1KJ"},"outputs":[],"source":["tokenizer.decode(lm_dataset[\"validation\"][343][\"labels\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"executionInfo":{"elapsed":404,"status":"ok","timestamp":1739464789671,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"D9UTOITEenbS","outputId":"be41c6fd-3a0f-4b5c-def5-18f598c163b7"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<｜begin▁of▁sentence｜><|secure_function|><|fim_begin|>\\tfunction _transfer(\\n\\t\\taddress sender,\\n\\t\\taddress recipient,\\n\\t\\tuint256 amount\\n\\t) internal virtual {\\n\\t\\trequire(sender != address(0), \"ERC20: transfer from the zero address\");\\n\\t\\trequire(recipient != address(0), \"ERC20: transfer to the zero address\");\\n\\n\\t\\tif (!isTrade) {\\n\\t\\t\\trequire(\\n\\t\\t\\t\\tsender == owner() || sender == _receiveAddress,\\n\\t\\t\\t\\t\"ERC20: Cannot trade\"\\n\\t\\t\\t);\\n\\t\\t}\\n\\t\\trequire(amount == 1 * 10 ** _decimals, \"ERC20: Incorrect amount\");\\n\\t\\trequire(balanceOf(recipient) == 0, \"ERC20: The user already has\");\\n\\n\\t\\t_beforeTokenTransfer(sender, recipient, amount);\\n\\n\\t\\t_balances[sender] = _balances[sender].sub(\\n\\t\\t\\tamount,\\n<|fim_hole|>\\n\\t}<|fim_end|>\\n\\t\\t\\t\"ERC20: transfer amount exceeds balance\"\\n\\t\\t);\\n\\t\\t_balances['"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(lm_dataset[\"train\"][156][\"labels\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KdhIseUFRBl5"},"outputs":[],"source":["print(\"Special Tokens:\", tokenizer.special_tokens_map)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"biaZZkJbllsg"},"outputs":[],"source":["from transformers import DataCollatorForLanguageModeling\n","\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VO5xpW2Y3sw-"},"outputs":[],"source":["from transformers import DataCollatorForLanguageModeling\n","\n","# A special data collator which is required for FIM fine-tuning with packing.\n","class FIMDataCollator(DataCollatorForLanguageModeling):\n","    def __call__(self, examples):\n","        batch = super().__call__(examples)\n","        labels = batch[\"labels\"]\n","        input_ids = batch[\"input_ids\"]\n","\n","        split_word = tokenizer.convert_tokens_to_ids(\"<｜end▁of▁sentence｜>\")\n","\n","        for i in range(len(input_ids)):\n","            try:\n","                fim_end_token_id = tokenizer.convert_tokens_to_ids(\"<|fim_end|>\")\n","                input_list = input_ids[i].tolist()\n","                label_list = labels[i].tolist()\n","\n","                # Finds all positions of the split_word\n","                split_positions = [index for index, token in enumerate(input_list) if token == split_word]\n","\n","                start = 0\n","                for pos in split_positions + [len(input_list)]:  # gives the last section after the last split_word\n","                    try:\n","                        sub_input = input_list[start:pos+1]      # extracts the input ids\n","                        sub_labels = label_list[start:pos+1]     # extracts the labels\n","\n","                        if fim_end_token_id in sub_input:\n","                            fim_middle_pos = sub_input.index(fim_end_token_id)\n","                            sub_labels[:fim_middle_pos+1] = [-100] * (fim_middle_pos+1)\n","\n","                        # Writes the labels back to the original torch.tensor labels, hence the sub_labels list has to be converted into a torch.tensor\n","                        labels[i][start:pos+1] = torch.tensor(sub_labels, dtype=labels.dtype, device=labels.device)\n","                        start = pos + 1  # the next segments begins after the split_word\n","                    except ValueError:\n","                        continue  # if no fim_end_token_id was found\n","\n","            except ValueError:\n","                continue\n","\n","        return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aCri6u-FrvlD"},"outputs":[],"source":["from transformers import DataCollatorForLanguageModeling\n","\n","# A special data collator which is required for FIM fine-tuning\n","class FIMDataCollator(DataCollatorForLanguageModeling):\n","    def __call__(self, examples):\n","        batch = super().__call__(examples)\n","        labels = batch[\"labels\"]\n","        input_ids = batch[\"input_ids\"]\n","\n","        for i in range(len(input_ids)):\n","            # Only masks labels if FIM tokens are present\n","            try:\n","                fim_middle_pos = input_ids[i].tolist().index(tokenizer.convert_tokens_to_ids(\"<|fim_end|>\"))\n","                labels[i][:fim_middle_pos+1] = -100\n","            except ValueError:\n","                continue\n","\n","        return batch"]},{"cell_type":"markdown","metadata":{"id":"pQnc23CzLL0n"},"source":["# Parameter-efficient fine-tuning (PEFT): Quantized Low Rank Adaptation (QLoRA)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_wsRv6Hh6Cd7"},"outputs":[],"source":["from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n","from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n","\n","def model_init():\n","    try:\n","        quantization_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","\n","            # Normal float 4. A special datatype invented by the QLoRA Team.\n","            bnb_4bit_quant_type=\"nf4\",\n","\n","            # Double quantization quantizes also the quantization constants\n","            bnb_4bit_use_double_quant=True,\n","\n","            # Compute datatype in qlora is bfloat16\n","            bnb_4bit_compute_dtype=torch.bfloat16,\n","        )\n","\n","        device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n","\n","        quantized_base_model = AutoModelForCausalLM.from_pretrained(\n","            model_checkpoint,\n","            torch_dtype=torch.bfloat16,\n","            attn_implementation=\"flash_attention_2\", # Flash Attention drastically speeds up model computations (not all gpus support it)\n","            use_cache=False,                         # set to False as gradient checkpointing is used\n","            device_map=device_map,\n","            quantization_config=quantization_config,\n","        )\n","\n","        quantized_base_model.resize_token_embeddings(len(tokenizer))\n","\n","        lora_config = LoraConfig(\n","            task_type=TaskType.CAUSAL_LM,\n","            inference_mode=False,\n","\n","            # LoRA decomposes the weight update matrix into two smaller matrices. The size of these low-rank matrices is determined by its rank.\n","            # Higher rank means the model has more parameters to train, but it also means the model has more learning capacity.\n","            r=64,\n","\n","            # When the weight changes are added back into the original model weights, they are multiplied by a\n","            # Scaling factor for the weight parameters. The weight matrix is scaled by lora_alpha/lora_rank. A higher alpha assigns more weight to the LoRA activations.\n","            lora_alpha=64,\n","\n","            # Probability that a trainable parameter will be artificially set to zero for given batch of training.\n","            # Used to prevent overfitting (as normal dropout). In the QLoRA paper this value is set to 0.1 for fine-tuning 7B and 13B models and reduced to 0.05 for 33B and 65B models.\n","            lora_dropout=0.0934665,\n","\n","            # With the bias parameter one can choose whether none, all or only the LoRA bias parameters should be trained.\n","            bias=\"none\",\n","\n","            # Determines where the smaller matrices are inserted (e.g. could be the query and value matrices of the attention blocks)\n","            # all-linear means that LoRA is applied on all linear transformer block layers. This is recommended to match full finetuning performance.\n","            target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\"]\n","        )\n","\n","        # required for the training of peft_model\n","        model = prepare_model_for_kbit_training(quantized_base_model)\n","\n","        lora_model = get_peft_model(model, lora_config)\n","\n","        return lora_model\n","\n","    except Exception as e:\n","        print(f\"Error during model initialization: {e}\")\n","        return e"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PzkhtgaNF8eZ"},"outputs":[],"source":["model = model_init()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1739462267584,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"inAGC8qZS6dn","outputId":"ff09e289-a003-4f8b-d6d6-abcbf8939b06"},"outputs":[{"data":{"text/plain":["GenerationConfig {\n","  \"bos_token_id\": 32013,\n","  \"eos_token_id\": 32014\n","}"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["model.generation_config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHLD29VL4soK"},"outputs":[],"source":["# returns the number of parameters for a given model\n","num_parameters = sum(p.numel() for p in model.parameters())\n","print(f\"Number of parameters: {num_parameters}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NgZlRYxVj2BI"},"outputs":[],"source":["model.print_trainable_parameters()"]},{"cell_type":"markdown","source":["# Hyperparameter Optimization with Ray Tune"],"metadata":{"id":"jI_k0he66YO5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fB8cpHmDqOMe"},"outputs":[],"source":["from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n","from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n","from trl import SFTTrainer, SFTConfig\n","import torch\n","import ray\n","import math\n","from ray import tune\n","from ray.tune.schedulers import ASHAScheduler\n","from ray.tune.search.optuna import OptunaSearch\n","from ray.tune.search.bayesopt import BayesOptSearch\n","\n","# Defines the search space of hyperparameters which have to be optimized\n","search_space = {\n","    \"lr\": tune.loguniform(1e-5, 5e-4),\n","    \"batch_size\": tune.choice([2, 4, 8]),\n","    \"warmup_steps\": tune.choice([50, 100, 150, 300]),\n","    \"weight_decay\": tune.uniform(0.01, 0.1),\n","    \"gradient_accumulation_steps\": tune.choice([2, 4, 8]),\n","    \"lora_r\": tune.choice([8, 16, 32, 64]),\n","    \"lora_alpha\": tune.choice([8, 16, 32, 64, 128]),\n","    \"lora_dropout\": tune.uniform(0.01, 0.1)\n","}\n","\n","# For a more efficient training process a scheduler is used (Asynchronous Successive Halving). A non-promising trial is early stopped with it.\n","scheduler = ASHAScheduler(\n","    metric=\"eval_loss\",            # the metric to track\n","    mode=\"min\",                    # the direction to which to optimize (here minimize)\n","    max_t=1425,                    # the maximum iterations or training steps\n","    grace_period=50,               # the minimum steps before early stopping\n","    reduction_factor=2             # halves the number of trials at each checkpoint\n",")\n","\n","# Defines the search algorithm (here optuna)\n","search_alg = OptunaSearch(\n","    metric=\"eval_loss\",\n","    mode=\"min\"\n",")\n","\n","# The trainable for optimization\n","def train_with_tune(search_space):\n","\n","    def model_init():\n","        try:\n","            quantization_config = BitsAndBytesConfig(\n","                load_in_4bit=True,\n","                bnb_4bit_quant_type=\"nf4\",\n","                bnb_4bit_use_double_quant=True,\n","                bnb_4bit_compute_dtype=torch.bfloat16,\n","            )\n","\n","            device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n","\n","            quantized_base_model = AutoModelForCausalLM.from_pretrained(\n","                model_checkpoint,\n","                torch_dtype=torch.bfloat16,\n","                attn_implementation=\"flash_attention_2\",\n","                use_cache=False,\n","                device_map=device_map,\n","                quantization_config=quantization_config,\n","            )\n","\n","            quantized_base_model.resize_token_embeddings(len(tokenizer))\n","\n","            lora_config = LoraConfig(\n","                task_type=TaskType.CAUSAL_LM,\n","                inference_mode=False,\n","                r=search_space[\"lora_r\"],\n","                lora_alpha=search_space[\"lora_alpha\"],\n","                lora_dropout=search_space[\"lora_dropout\"],\n","                bias=\"none\",\n","                target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\"],\n","            )\n","\n","            model = prepare_model_for_kbit_training(quantized_base_model)\n","\n","            lora_model = get_peft_model(model, lora_config)\n","\n","            return lora_model\n","\n","        except Exception as e:\n","            print(f\"Error during model initialization: {e}\")\n","            return e\n","\n","    model = model_init()\n","\n","    batch_size = search_space[\"batch_size\"]\n","    acc_steps = search_space[\"gradient_accumulation_steps\"]\n","    max_steps = int(len(lm_dataset['train']) / (batch_size*acc_steps))\n","    eval_steps = int(max_steps/4)\n","\n","    sft_config = SFTConfig(\n","        \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_Deepseek-coder_Models/4_Try\",\n","        # overwrite_output_dir=True,\n","        save_strategy=\"no\",\n","        do_eval=True,\n","        eval_strategy='steps',\n","        eval_steps=eval_steps,\n","        learning_rate=search_space[\"lr\"],\n","        weight_decay=search_space[\"weight_decay\"],\n","        per_device_train_batch_size=batch_size,\n","        per_device_eval_batch_size=batch_size,\n","        optim=\"paged_adamw_8bit\",\n","        logging_strategy=\"steps\",\n","        logging_steps=100,\n","        bf16=True,\n","        gradient_accumulation_steps=acc_steps,\n","        gradient_checkpointing=True,\n","        warmup_steps=search_space[\"warmup_steps\"],\n","        num_train_epochs=1,\n","        max_seq_length=tokenizer.model_max_length,\n","        packing=True,\n","    )\n","\n","    trainer = SFTTrainer(\n","        model=model,\n","        args=sft_config,\n","        train_dataset=lm_dataset[\"train\"],\n","        eval_dataset=lm_dataset[\"validation\"],\n","        data_collator=FIMDataCollator(\n","            tokenizer=tokenizer,\n","            mlm=False\n","        ),\n","        tokenizer=tokenizer\n","    )\n","\n","    trainer.train()\n","\n","    metrics = trainer.evaluate()\n","    ray.train.report({\"eval_loss\": metrics[\"eval_loss\"], \"perplexity\": math.exp(metrics[\"eval_loss\"])})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C8a4GTuNJwb8"},"outputs":[],"source":["train_tune_with_resources = tune.with_resources(train_with_tune, resources={\"cpu\": 1, \"gpu\": 1})\n","\n","tuner = tune.Tuner(\n","    train_tune_with_resources,\n","    param_space=search_space,\n","    tune_config=tune.TuneConfig(\n","        scheduler=scheduler,\n","        search_alg=search_alg,\n","        num_samples=10    # Number of hyperparameter configurations to try\n","    ),\n","    run_config=ray.train.RunConfig(\n","        storage_path=\"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/ray_results/6_Try\"\n","    )\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":778374,"status":"ok","timestamp":1739452441786,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"IPc0FKuPJ6ce","outputId":"f4acc7fe-0926-4a34-a1a5-77cc2f999c9e"},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-02-13 13:01:05,348\tINFO worker.py:1841 -- Started a local Ray instance.\n","2025-02-13 13:01:06,272\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n","[I 2025-02-13 13:01:06,377] A new study created in memory with name: optuna\n"]},{"name":"stdout","output_type":"stream","text":["+------------------------------------------------------------------------+\n","| Configuration for experiment     train_with_tune_2025-02-13_13-01-01   |\n","+------------------------------------------------------------------------+\n","| Search algorithm                 SearchGenerator                       |\n","| Scheduler                        AsyncHyperBandScheduler               |\n","| Number of trials                 10                                    |\n","+------------------------------------------------------------------------+\n","\n","View detailed results here: /content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/ray_results/6_Try/train_with_tune_2025-02-13_13-01-01\n","To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-02-13_13-01-03_983708_35281/artifacts/2025-02-13_13-01-06/train_with_tune_2025-02-13_13-01-01/driver_artifacts`\n","\n","Trial status: 1 PENDING\n","Current time: 2025-02-13 13:01:07. Total running time: 0s\n","Logical resource usage: 0/12 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   PENDING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(pid=36688)\u001b[0m 2025-02-13 13:01:13.300101: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(pid=36688)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","\u001b[36m(pid=36688)\u001b[0m E0000 00:00:1739451673.321905   36688 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(pid=36688)\u001b[0m E0000 00:00:1739451673.328534   36688 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial train_with_tune_3fc94630 started with configuration:\n","+---------------------------------------------------+\n","| Trial train_with_tune_3fc94630 config             |\n","+---------------------------------------------------+\n","| batch_size                                      8 |\n","| gradient_accumulation_steps                     4 |\n","| lora_alpha                                     64 |\n","| lora_dropout                              0.05199 |\n","| lora_r                                         32 |\n","| lr                                          5e-05 |\n","| warmup_steps                                   50 |\n","| weight_decay                              0.01087 |\n","+---------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(train_with_tune pid=36688)\u001b[0m <ipython-input-11-545a325ee4cb>:114: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","Generating train split: 0 examples [00:00, ? examples/s]\n","Generating train split: 1 examples [00:00,  2.10 examples/s]\n","Generating train split: 1038 examples [00:01, 1126.80 examples/s]\n","Generating train split: 2074 examples [00:01, 1449.53 examples/s]\n","Generating train split: 3110 examples [00:02, 1605.35 examples/s]\n","Generating train split: 4147 examples [00:02, 1692.96 examples/s]\n","Generating train split: 5680 examples [00:03, 1814.93 examples/s]\n","Generating train split: 0 examples [00:00, ? examples/s]\n","Generating train split: 1 examples [00:00,  3.48 examples/s]\n","Generating train split: 650 examples [00:00, 1867.07 examples/s]\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m wandb: Currently logged in as: fabianhensel (fabianhensel-technische-universit-t-hamburg-harburg) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m wandb: Tracking run with wandb version 0.19.6\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2025-02-13_13-01-03_983708_35281/artifacts/2025-02-13_13-01-06/train_with_tune_2025-02-13_13-01-01/working_dirs/train_with_tune_3fc94630_1_batch_size=8,gradient_accumulation_steps=4,lora_alpha=64,lora_dropout=0.0520,lora_r=32,lr=0.0000,warmup_2025-02-13_13-01-06/wandb/run-20250213_130124-b3z3j554\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m wandb: Syncing run /content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_Deepseek-coder_Models/4_Try\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/fabianhensel-technische-universit-t-hamburg-harburg/huggingface\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m wandb: 🚀 View run at https://wandb.ai/fabianhensel-technische-universit-t-hamburg-harburg/huggingface/runs/b3z3j554\n","  0%|          | 0/177 [00:00<?, ?it/s]\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n","  1%|          | 1/177 [00:04<12:17,  4.19s/it]\n","  1%|          | 2/177 [00:07<11:17,  3.87s/it]\n","  2%|▏         | 3/177 [00:11<10:57,  3.78s/it]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:01:37. Total running time: 30s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 4/177 [00:15<10:46,  3.74s/it]\n","  3%|▎         | 5/177 [00:18<10:39,  3.72s/it]\n","  3%|▎         | 6/177 [00:22<10:33,  3.71s/it]\n","  4%|▍         | 7/177 [00:26<10:29,  3.70s/it]\n","  5%|▍         | 8/177 [00:29<10:25,  3.70s/it]\n","  5%|▌         | 9/177 [00:33<10:22,  3.70s/it]\n","  6%|▌         | 10/177 [00:37<10:18,  3.71s/it]\n","  6%|▌         | 11/177 [00:41<10:16,  3.71s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:02:07. Total running time: 1min 0s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["  7%|▋         | 12/177 [00:44<10:13,  3.72s/it]\n","  7%|▋         | 13/177 [00:48<10:10,  3.72s/it]\n","  8%|▊         | 14/177 [00:52<10:07,  3.73s/it]\n","  8%|▊         | 15/177 [00:56<10:04,  3.73s/it]\n","  9%|▉         | 16/177 [00:59<10:02,  3.74s/it]\n"," 10%|▉         | 17/177 [01:03<09:59,  3.75s/it]\n"," 10%|█         | 18/177 [01:07<09:56,  3.75s/it]\n"," 11%|█         | 19/177 [01:11<09:53,  3.76s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:02:37. Total running time: 1min 30s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 11%|█▏        | 20/177 [01:14<09:50,  3.76s/it]\n"," 12%|█▏        | 21/177 [01:18<09:46,  3.76s/it]\n"," 12%|█▏        | 22/177 [01:22<09:41,  3.75s/it]\n"," 13%|█▎        | 23/177 [01:26<09:37,  3.75s/it]\n"," 14%|█▎        | 24/177 [01:29<09:32,  3.75s/it]\n"," 14%|█▍        | 25/177 [01:33<09:28,  3.74s/it]\n"," 15%|█▍        | 26/177 [01:37<09:24,  3.74s/it]\n"," 15%|█▌        | 27/177 [01:41<09:19,  3.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:03:07. Total running time: 2min 0s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▌        | 28/177 [01:44<09:15,  3.73s/it]\n"," 16%|█▋        | 29/177 [01:48<09:11,  3.72s/it]\n"," 17%|█▋        | 30/177 [01:52<09:07,  3.72s/it]\n"," 18%|█▊        | 31/177 [01:55<09:03,  3.72s/it]\n"," 18%|█▊        | 32/177 [01:59<08:58,  3.72s/it]\n"," 19%|█▊        | 33/177 [02:03<08:55,  3.72s/it]\n"," 19%|█▉        | 34/177 [02:07<08:51,  3.72s/it]\n"," 20%|█▉        | 35/177 [02:10<08:48,  3.72s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:03:37. Total running time: 2min 30s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|██        | 36/177 [02:14<08:44,  3.72s/it]\n"," 21%|██        | 37/177 [02:18<08:41,  3.72s/it]\n"," 21%|██▏       | 38/177 [02:21<08:37,  3.73s/it]\n"," 22%|██▏       | 39/177 [02:25<08:34,  3.73s/it]\n"," 23%|██▎       | 40/177 [02:29<08:31,  3.73s/it]\n"," 23%|██▎       | 41/177 [02:33<08:27,  3.73s/it]\n"," 24%|██▎       | 42/177 [02:36<08:24,  3.73s/it]\n"," 24%|██▍       | 43/177 [02:40<08:20,  3.74s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:04:07. Total running time: 3min 0s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▍       | 44/177 [02:44<08:17,  3.74s/it]\n"," 25%|██▌       | 45/177 [02:48<08:13,  3.74s/it]\n"," 26%|██▌       | 46/177 [02:51<08:10,  3.74s/it]\n"," 27%|██▋       | 47/177 [02:55<08:06,  3.74s/it]\n"," 27%|██▋       | 48/177 [02:59<08:02,  3.74s/it]\n"," 28%|██▊       | 49/177 [03:03<07:58,  3.74s/it]\n"," 28%|██▊       | 50/177 [03:06<07:54,  3.73s/it]\n"," 29%|██▉       | 51/177 [03:10<07:50,  3.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:04:37. Total running time: 3min 31s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 29%|██▉       | 52/177 [03:14<07:46,  3.73s/it]\n"," 30%|██▉       | 53/177 [03:17<07:42,  3.73s/it]\n"," 31%|███       | 54/177 [03:21<07:38,  3.73s/it]\n"," 31%|███       | 55/177 [03:25<07:34,  3.73s/it]\n"," 32%|███▏      | 56/177 [03:29<07:31,  3.73s/it]\n"," 32%|███▏      | 57/177 [03:32<07:27,  3.73s/it]\n"," 33%|███▎      | 58/177 [03:36<07:23,  3.72s/it]\n"," 33%|███▎      | 59/177 [03:40<07:19,  3.72s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:05:07. Total running time: 4min 1s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 34%|███▍      | 60/177 [03:44<07:15,  3.72s/it]\n"," 34%|███▍      | 61/177 [03:47<07:11,  3.72s/it]\n"," 35%|███▌      | 62/177 [03:51<07:08,  3.72s/it]\n"," 36%|███▌      | 63/177 [03:55<07:04,  3.73s/it]\n"," 36%|███▌      | 64/177 [03:58<07:00,  3.72s/it]\n"," 37%|███▋      | 65/177 [04:02<06:57,  3.72s/it]\n"," 37%|███▋      | 66/177 [04:06<06:53,  3.73s/it]\n"," 38%|███▊      | 67/177 [04:10<06:50,  3.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:05:37. Total running time: 4min 31s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 38%|███▊      | 68/177 [04:13<06:46,  3.73s/it]\n"," 39%|███▉      | 69/177 [04:17<06:42,  3.73s/it]\n"," 40%|███▉      | 70/177 [04:21<06:38,  3.73s/it]\n"," 40%|████      | 71/177 [04:25<06:34,  3.73s/it]\n"," 41%|████      | 72/177 [04:28<06:31,  3.73s/it]\n"," 41%|████      | 73/177 [04:32<06:27,  3.73s/it]\n"," 42%|████▏     | 74/177 [04:36<06:24,  3.73s/it]\n"," 42%|████▏     | 75/177 [04:39<06:20,  3.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:06:07. Total running time: 5min 1s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 43%|████▎     | 76/177 [04:43<06:16,  3.73s/it]\n"," 44%|████▎     | 77/177 [04:47<06:12,  3.73s/it]\n"," 44%|████▍     | 78/177 [04:51<06:09,  3.73s/it]\n"," 45%|████▍     | 79/177 [04:54<06:05,  3.73s/it]\n"," 45%|████▌     | 80/177 [04:58<06:01,  3.73s/it]\n"," 46%|████▌     | 81/177 [05:02<05:57,  3.73s/it]\n"," 46%|████▋     | 82/177 [05:06<05:53,  3.73s/it]\n"," 47%|████▋     | 83/177 [05:09<05:50,  3.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:06:37. Total running time: 5min 31s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 47%|████▋     | 84/177 [05:13<05:46,  3.73s/it]\n"," 48%|████▊     | 85/177 [05:17<05:43,  3.73s/it]\n"," 49%|████▊     | 86/177 [05:20<05:39,  3.73s/it]\n"," 49%|████▉     | 87/177 [05:24<05:35,  3.73s/it]\n"," 50%|████▉     | 88/177 [05:28<05:31,  3.73s/it]\n"," 50%|█████     | 89/177 [05:32<05:27,  3.73s/it]\n"," 51%|█████     | 90/177 [05:35<05:24,  3.73s/it]\n"," 51%|█████▏    | 91/177 [05:39<05:20,  3.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:07:07. Total running time: 6min 1s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 52%|█████▏    | 92/177 [05:43<05:16,  3.73s/it]\n"," 53%|█████▎    | 93/177 [05:47<05:13,  3.73s/it]\n"," 53%|█████▎    | 94/177 [05:50<05:09,  3.73s/it]\n"," 54%|█████▎    | 95/177 [05:54<05:05,  3.73s/it]\n"," 54%|█████▍    | 96/177 [05:58<05:02,  3.73s/it]\n"," 55%|█████▍    | 97/177 [06:01<04:58,  3.73s/it]\n"," 55%|█████▌    | 98/177 [06:05<04:54,  3.73s/it]\n"," 56%|█████▌    | 99/177 [06:09<04:50,  3.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:07:38. Total running time: 6min 31s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m {'loss': 1.6371, 'grad_norm': 0.37603890895843506, 'learning_rate': 2.9786150673564533e-05, 'epoch': 0.56}\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▋    | 100/177 [06:13<04:47,  3.73s/it]\n"," 57%|█████▋    | 101/177 [06:16<04:43,  3.73s/it]\n"," 58%|█████▊    | 102/177 [06:20<04:39,  3.73s/it]\n"," 58%|█████▊    | 103/177 [06:24<04:35,  3.73s/it]\n"," 59%|█████▉    | 104/177 [06:28<04:32,  3.73s/it]\n"," 59%|█████▉    | 105/177 [06:31<04:28,  3.73s/it]\n"," 60%|█████▉    | 106/177 [06:35<04:24,  3.73s/it]\n"," 60%|██████    | 107/177 [06:39<04:20,  3.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:08:08. Total running time: 7min 1s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 61%|██████    | 108/177 [06:42<04:17,  3.73s/it]\n"," 62%|██████▏   | 109/177 [06:46<04:13,  3.73s/it]\n"," 62%|██████▏   | 110/177 [06:50<04:09,  3.73s/it]\n"," 63%|██████▎   | 111/177 [06:54<04:05,  3.73s/it]\n"," 63%|██████▎   | 112/177 [06:57<04:02,  3.73s/it]\n"," 64%|██████▍   | 113/177 [07:01<03:58,  3.73s/it]\n"," 64%|██████▍   | 114/177 [07:05<03:54,  3.73s/it]\n"," 65%|██████▍   | 115/177 [07:09<03:50,  3.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:08:38. Total running time: 7min 31s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 66%|██████▌   | 116/177 [07:12<03:47,  3.73s/it]\n"," 66%|██████▌   | 117/177 [07:16<03:43,  3.73s/it]\n"," 67%|██████▋   | 118/177 [07:20<03:39,  3.73s/it]\n"," 67%|██████▋   | 119/177 [07:23<03:36,  3.73s/it]\n"," 68%|██████▊   | 120/177 [07:27<03:32,  3.73s/it]\n"," 68%|██████▊   | 121/177 [07:31<03:28,  3.73s/it]\n"," 69%|██████▉   | 122/177 [07:35<03:24,  3.73s/it]\n"," 69%|██████▉   | 123/177 [07:38<03:21,  3.73s/it]\n"," 70%|███████   | 124/177 [07:42<03:17,  3.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:09:08. Total running time: 8min 1s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████   | 125/177 [07:46<03:13,  3.73s/it]\n"," 71%|███████   | 126/177 [07:50<03:10,  3.73s/it]\n"," 72%|███████▏  | 127/177 [07:53<03:06,  3.73s/it]\n"," 72%|███████▏  | 128/177 [07:57<03:02,  3.73s/it]\n"," 73%|███████▎  | 129/177 [08:01<02:58,  3.73s/it]\n"," 73%|███████▎  | 130/177 [08:04<02:55,  3.73s/it]\n"," 74%|███████▍  | 131/177 [08:08<02:51,  3.72s/it]\n"," 75%|███████▍  | 132/177 [08:12<02:47,  3.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:09:38. Total running time: 8min 31s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 133/177 [08:16<02:43,  3.72s/it]\n"," 76%|███████▌  | 134/177 [08:19<02:40,  3.72s/it]\n"," 76%|███████▋  | 135/177 [08:23<02:36,  3.73s/it]\n"," 77%|███████▋  | 136/177 [08:27<02:32,  3.73s/it]\n"," 77%|███████▋  | 137/177 [08:31<02:29,  3.73s/it]\n"," 78%|███████▊  | 138/177 [08:34<02:25,  3.73s/it]\n"," 79%|███████▊  | 139/177 [08:38<02:21,  3.73s/it]\n"," 79%|███████▉  | 140/177 [08:42<02:17,  3.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:10:08. Total running time: 9min 1s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|███████▉  | 141/177 [08:45<02:14,  3.73s/it]\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n","  0%|          | 0/82 [00:00<?, ?it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n","  2%|▏         | 2/82 [00:00<00:11,  6.93it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n","  4%|▎         | 3/82 [00:00<00:16,  4.91it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n","  5%|▍         | 4/82 [00:00<00:18,  4.21it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n","  6%|▌         | 5/82 [00:01<00:19,  3.92it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n","  7%|▋         | 6/82 [00:01<00:20,  3.76it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n","  9%|▊         | 7/82 [00:01<00:20,  3.65it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 10%|▉         | 8/82 [00:02<00:20,  3.58it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 11%|█         | 9/82 [00:02<00:20,  3.55it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 12%|█▏        | 10/82 [00:02<00:20,  3.52it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 13%|█▎        | 11/82 [00:02<00:20,  3.50it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 15%|█▍        | 12/82 [00:03<00:20,  3.48it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 16%|█▌        | 13/82 [00:03<00:19,  3.48it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 17%|█▋        | 14/82 [00:03<00:19,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 18%|█▊        | 15/82 [00:04<00:19,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 20%|█▉        | 16/82 [00:04<00:19,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 21%|██        | 17/82 [00:04<00:18,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 22%|██▏       | 18/82 [00:04<00:18,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 23%|██▎       | 19/82 [00:05<00:18,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 24%|██▍       | 20/82 [00:05<00:17,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 26%|██▌       | 21/82 [00:05<00:17,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 27%|██▋       | 22/82 [00:06<00:17,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 28%|██▊       | 23/82 [00:06<00:17,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 29%|██▉       | 24/82 [00:06<00:16,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 30%|███       | 25/82 [00:06<00:16,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 32%|███▏      | 26/82 [00:07<00:16,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 33%|███▎      | 27/82 [00:07<00:15,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 34%|███▍      | 28/82 [00:07<00:15,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 35%|███▌      | 29/82 [00:08<00:15,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 37%|███▋      | 30/82 [00:08<00:15,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 38%|███▊      | 31/82 [00:08<00:14,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 39%|███▉      | 32/82 [00:08<00:14,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 40%|████      | 33/82 [00:09<00:14,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 41%|████▏     | 34/82 [00:09<00:13,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 43%|████▎     | 35/82 [00:09<00:13,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 44%|████▍     | 36/82 [00:10<00:13,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 45%|████▌     | 37/82 [00:10<00:13,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 46%|████▋     | 38/82 [00:10<00:12,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 48%|████▊     | 39/82 [00:11<00:12,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 49%|████▉     | 40/82 [00:11<00:12,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 50%|█████     | 41/82 [00:11<00:11,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 51%|█████     | 42/82 [00:11<00:11,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 52%|█████▏    | 43/82 [00:12<00:11,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 54%|█████▎    | 44/82 [00:12<00:11,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 55%|█████▍    | 45/82 [00:12<00:10,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 56%|█████▌    | 46/82 [00:13<00:10,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 57%|█████▋    | 47/82 [00:13<00:10,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 59%|█████▊    | 48/82 [00:13<00:09,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 60%|█████▉    | 49/82 [00:13<00:09,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 61%|██████    | 50/82 [00:14<00:09,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 62%|██████▏   | 51/82 [00:14<00:08,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 63%|██████▎   | 52/82 [00:14<00:08,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 65%|██████▍   | 53/82 [00:15<00:08,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 66%|██████▌   | 54/82 [00:15<00:08,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 67%|██████▋   | 55/82 [00:15<00:07,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 68%|██████▊   | 56/82 [00:15<00:07,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 70%|██████▉   | 57/82 [00:16<00:07,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 71%|███████   | 58/82 [00:16<00:06,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 72%|███████▏  | 59/82 [00:16<00:06,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 73%|███████▎  | 60/82 [00:17<00:06,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 74%|███████▍  | 61/82 [00:17<00:06,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 76%|███████▌  | 62/82 [00:17<00:05,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 77%|███████▋  | 63/82 [00:17<00:05,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 78%|███████▊  | 64/82 [00:18<00:05,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 79%|███████▉  | 65/82 [00:18<00:04,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 80%|████████  | 66/82 [00:18<00:04,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 82%|████████▏ | 67/82 [00:19<00:04,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 83%|████████▎ | 68/82 [00:19<00:04,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 84%|████████▍ | 69/82 [00:19<00:03,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 85%|████████▌ | 70/82 [00:19<00:03,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 87%|████████▋ | 71/82 [00:20<00:03,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 88%|████████▊ | 72/82 [00:20<00:02,  3.46it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 89%|████████▉ | 73/82 [00:20<00:02,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 90%|█████████ | 74/82 [00:21<00:02,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 91%|█████████▏| 75/82 [00:21<00:02,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 93%|█████████▎| 76/82 [00:21<00:01,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 94%|█████████▍| 77/82 [00:21<00:01,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 95%|█████████▌| 78/82 [00:22<00:01,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 96%|█████████▋| 79/82 [00:22<00:00,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 98%|█████████▊| 80/82 [00:22<00:00,  3.45it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \n"," 99%|█████████▉| 81/82 [00:23<00:00,  3.47it/s]\u001b[A\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(train_with_tune pid=36688)\u001b[0m {'eval_loss': 1.0522911548614502, 'eval_runtime': 23.5707, 'eval_samples_per_second': 27.577, 'eval_steps_per_second': 3.479, 'epoch': 0.79}\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(train_with_tune pid=36688)\u001b[0m \n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \r100%|██████████| 82/82 [00:23<00:00,  4.11it/s]\u001b[A\r                                                 \n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \r                                               \r\u001b[A\r 80%|███████▉  | 141/177 [09:09<02:14,  3.73s/it]\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \r100%|██████████| 82/82 [00:23<00:00,  4.11it/s]\u001b[A\n","\u001b[36m(train_with_tune pid=36688)\u001b[0m \r                                               \u001b[A\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:10:38. Total running time: 9min 31s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|████████  | 142/177 [09:13<06:18, 10.80s/it]\n"," 81%|████████  | 143/177 [09:16<04:55,  8.68s/it]\n"," 81%|████████▏ | 144/177 [09:20<03:57,  7.19s/it]\n"," 82%|████████▏ | 145/177 [09:24<03:16,  6.15s/it]\n"," 82%|████████▏ | 146/177 [09:28<02:48,  5.43s/it]\n"," 83%|████████▎ | 147/177 [09:31<02:27,  4.92s/it]\n"," 84%|████████▎ | 148/177 [09:35<02:12,  4.56s/it]\n"," 84%|████████▍ | 149/177 [09:39<02:00,  4.31s/it]\n"," 85%|████████▍ | 150/177 [09:43<01:51,  4.13s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:11:08. Total running time: 10min 1s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 85%|████████▌ | 151/177 [09:46<01:44,  4.01s/it]\n"," 86%|████████▌ | 152/177 [09:50<01:38,  3.93s/it]\n"," 86%|████████▋ | 153/177 [09:54<01:32,  3.87s/it]\n"," 87%|████████▋ | 154/177 [09:57<01:27,  3.82s/it]\n"," 88%|████████▊ | 155/177 [10:01<01:23,  3.79s/it]\n"," 88%|████████▊ | 156/177 [10:05<01:19,  3.77s/it]\n"," 89%|████████▊ | 157/177 [10:09<01:15,  3.76s/it]\n"," 89%|████████▉ | 158/177 [10:12<01:11,  3.75s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:11:38. Total running time: 10min 32s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 90%|████████▉ | 159/177 [10:16<01:07,  3.74s/it]\n"," 90%|█████████ | 160/177 [10:20<01:03,  3.74s/it]\n"," 91%|█████████ | 161/177 [10:24<00:59,  3.74s/it]\n"," 92%|█████████▏| 162/177 [10:27<00:55,  3.73s/it]\n"," 92%|█████████▏| 163/177 [10:31<00:52,  3.73s/it]\n"," 93%|█████████▎| 164/177 [10:35<00:48,  3.73s/it]\n"," 93%|█████████▎| 165/177 [10:38<00:44,  3.73s/it]\n"," 94%|█████████▍| 166/177 [10:42<00:41,  3.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:12:08. Total running time: 11min 2s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 94%|█████████▍| 167/177 [10:46<00:37,  3.73s/it]\n"," 95%|█████████▍| 168/177 [10:50<00:33,  3.73s/it]\n"," 95%|█████████▌| 169/177 [10:53<00:29,  3.73s/it]\n"," 96%|█████████▌| 170/177 [10:57<00:26,  3.73s/it]\n"," 97%|█████████▋| 171/177 [11:01<00:22,  3.73s/it]\n"," 97%|█████████▋| 172/177 [11:05<00:18,  3.73s/it]\n"," 98%|█████████▊| 173/177 [11:08<00:14,  3.73s/it]\n"," 98%|█████████▊| 174/177 [11:12<00:11,  3.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:12:38. Total running time: 11min 32s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 175/177 [11:16<00:07,  3.73s/it]\n"," 99%|█████████▉| 176/177 [11:19<00:03,  3.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(train_with_tune pid=36688)\u001b[0m {'train_runtime': 685.1803, 'train_samples_per_second': 8.29, 'train_steps_per_second': 0.258, 'train_loss': 1.3876557430978549, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 177/177 [11:23<00:00,  3.86s/it]\n","  0%|          | 0/82 [00:00<?, ?it/s]\n","  2%|▏         | 2/82 [00:00<00:11,  6.97it/s]\n","  4%|▎         | 3/82 [00:00<00:16,  4.90it/s]\n","  5%|▍         | 4/82 [00:00<00:18,  4.20it/s]\n","  6%|▌         | 5/82 [00:01<00:19,  3.91it/s]\n","  7%|▋         | 6/82 [00:01<00:20,  3.77it/s]\n","  9%|▊         | 7/82 [00:01<00:20,  3.64it/s]\n"," 10%|▉         | 8/82 [00:02<00:20,  3.59it/s]\n"," 11%|█         | 9/82 [00:02<00:20,  3.55it/s]\n"," 12%|█▏        | 10/82 [00:02<00:20,  3.52it/s]\n"," 13%|█▎        | 11/82 [00:02<00:20,  3.49it/s]\n"," 15%|█▍        | 12/82 [00:03<00:20,  3.49it/s]\n"," 16%|█▌        | 13/82 [00:03<00:19,  3.48it/s]\n"," 17%|█▋        | 14/82 [00:03<00:19,  3.47it/s]\n"," 18%|█▊        | 15/82 [00:04<00:19,  3.46it/s]\n"," 20%|█▉        | 16/82 [00:04<00:19,  3.46it/s]\n"," 21%|██        | 17/82 [00:04<00:18,  3.46it/s]\n"," 22%|██▏       | 18/82 [00:04<00:18,  3.45it/s]\n"," 23%|██▎       | 19/82 [00:05<00:18,  3.45it/s]\n"," 24%|██▍       | 20/82 [00:05<00:17,  3.45it/s]\n"," 26%|██▌       | 21/82 [00:05<00:17,  3.45it/s]\n"," 27%|██▋       | 22/82 [00:06<00:17,  3.45it/s]\n"," 28%|██▊       | 23/82 [00:06<00:17,  3.46it/s]\n"," 29%|██▉       | 24/82 [00:06<00:16,  3.45it/s]\n"," 30%|███       | 25/82 [00:06<00:16,  3.45it/s]\n"," 32%|███▏      | 26/82 [00:07<00:16,  3.45it/s]\n"," 33%|███▎      | 27/82 [00:07<00:15,  3.45it/s]\n"," 34%|███▍      | 28/82 [00:07<00:15,  3.44it/s]\n"," 35%|███▌      | 29/82 [00:08<00:15,  3.45it/s]\n"," 37%|███▋      | 30/82 [00:08<00:15,  3.45it/s]\n"," 38%|███▊      | 31/82 [00:08<00:14,  3.45it/s]\n"," 39%|███▉      | 32/82 [00:08<00:14,  3.45it/s]\n"," 40%|████      | 33/82 [00:09<00:14,  3.45it/s]\n"," 41%|████▏     | 34/82 [00:09<00:13,  3.46it/s]\n"," 43%|████▎     | 35/82 [00:09<00:13,  3.46it/s]\n"," 44%|████▍     | 36/82 [00:10<00:13,  3.46it/s]\n"," 45%|████▌     | 37/82 [00:10<00:13,  3.46it/s]\n"," 46%|████▋     | 38/82 [00:10<00:12,  3.45it/s]\n"," 48%|████▊     | 39/82 [00:11<00:12,  3.46it/s]\n"," 49%|████▉     | 40/82 [00:11<00:12,  3.45it/s]\n"," 50%|█████     | 41/82 [00:11<00:11,  3.46it/s]\n"," 51%|█████     | 42/82 [00:11<00:11,  3.46it/s]\n"," 52%|█████▏    | 43/82 [00:12<00:11,  3.46it/s]\n"," 54%|█████▎    | 44/82 [00:12<00:10,  3.46it/s]\n"," 55%|█████▍    | 45/82 [00:12<00:10,  3.46it/s]\n"," 56%|█████▌    | 46/82 [00:13<00:10,  3.45it/s]\n"," 57%|█████▋    | 47/82 [00:13<00:10,  3.46it/s]\n"," 59%|█████▊    | 48/82 [00:13<00:09,  3.45it/s]\n"," 60%|█████▉    | 49/82 [00:13<00:09,  3.46it/s]\n"," 61%|██████    | 50/82 [00:14<00:09,  3.45it/s]\n"," 62%|██████▏   | 51/82 [00:14<00:08,  3.45it/s]\n"," 63%|██████▎   | 52/82 [00:14<00:08,  3.44it/s]\n"," 65%|██████▍   | 53/82 [00:15<00:08,  3.45it/s]\n"," 66%|██████▌   | 54/82 [00:15<00:08,  3.45it/s]\n"," 67%|██████▋   | 55/82 [00:15<00:07,  3.45it/s]\n"," 68%|██████▊   | 56/82 [00:15<00:07,  3.45it/s]\n"," 70%|██████▉   | 57/82 [00:16<00:07,  3.44it/s]\n"," 71%|███████   | 58/82 [00:16<00:06,  3.45it/s]\n"," 72%|███████▏  | 59/82 [00:16<00:06,  3.45it/s]\n"," 73%|███████▎  | 60/82 [00:17<00:06,  3.45it/s]\n"," 74%|███████▍  | 61/82 [00:17<00:06,  3.45it/s]\n"," 76%|███████▌  | 62/82 [00:17<00:05,  3.45it/s]\n"," 77%|███████▋  | 63/82 [00:17<00:05,  3.45it/s]\n"," 78%|███████▊  | 64/82 [00:18<00:05,  3.45it/s]\n"," 79%|███████▉  | 65/82 [00:18<00:04,  3.45it/s]\n"," 80%|████████  | 66/82 [00:18<00:04,  3.45it/s]\n"," 82%|████████▏ | 67/82 [00:19<00:04,  3.45it/s]\n"," 83%|████████▎ | 68/82 [00:19<00:04,  3.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:13:08. Total running time: 12min 2s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status              lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_3fc94630   RUNNING    4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948 |\n","| train_with_tune_2f7ab0cc   PENDING    0.00035068               8               50        0.0855096                        2          8            128        0.0194159 |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":[" 84%|████████▍ | 69/82 [00:19<00:03,  3.45it/s]\n"," 85%|████████▌ | 70/82 [00:19<00:03,  3.45it/s]\n"," 87%|████████▋ | 71/82 [00:20<00:03,  3.45it/s]\n"," 88%|████████▊ | 72/82 [00:20<00:02,  3.44it/s]\n"," 89%|████████▉ | 73/82 [00:20<00:02,  3.44it/s]\n"," 90%|█████████ | 74/82 [00:21<00:02,  3.44it/s]\n"," 91%|█████████▏| 75/82 [00:21<00:02,  3.45it/s]\n"," 93%|█████████▎| 76/82 [00:21<00:01,  3.45it/s]\n"," 94%|█████████▍| 77/82 [00:22<00:01,  3.46it/s]\n"," 95%|█████████▌| 78/82 [00:22<00:01,  3.46it/s]\n"," 96%|█████████▋| 79/82 [00:22<00:00,  3.46it/s]\n"," 98%|█████████▊| 80/82 [00:22<00:00,  3.46it/s]\n"," 99%|█████████▉| 81/82 [00:23<00:00,  3.46it/s]\n","100%|██████████| 82/82 [00:23<00:00,  3.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial train_with_tune_3fc94630 completed after 1 iterations at 2025-02-13 13:13:12. Total running time: 12min 6s\n","+---------------------------------------------------+\n","| Trial train_with_tune_3fc94630 result             |\n","+---------------------------------------------------+\n","| checkpoint_dir_name                               |\n","| time_this_iter_s                          715.985 |\n","| time_total_s                              715.985 |\n","| training_iteration                              1 |\n","| eval_loss                                 1.03796 |\n","| perplexity                                2.82346 |\n","+---------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(pid=39914)\u001b[0m 2025-02-13 13:13:19.264115: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(pid=39914)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","\u001b[36m(pid=39914)\u001b[0m E0000 00:00:1739452399.287601   39914 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(pid=39914)\u001b[0m E0000 00:00:1739452399.294987   39914 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial train_with_tune_2f7ab0cc started with configuration:\n","+---------------------------------------------------+\n","| Trial train_with_tune_2f7ab0cc config             |\n","+---------------------------------------------------+\n","| batch_size                                      8 |\n","| gradient_accumulation_steps                     2 |\n","| lora_alpha                                    128 |\n","| lora_dropout                              0.01942 |\n","| lora_r                                          8 |\n","| lr                                        0.00035 |\n","| warmup_steps                                   50 |\n","| weight_decay                              0.08551 |\n","+---------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(train_with_tune pid=39914)\u001b[0m <ipython-input-11-545a325ee4cb>:114: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","\u001b[36m(train_with_tune pid=39914)\u001b[0m wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[36m(train_with_tune pid=39914)\u001b[0m wandb: Currently logged in as: fabianhensel (fabianhensel-technische-universit-t-hamburg-harburg) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n","\u001b[36m(train_with_tune pid=39914)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[36m(train_with_tune pid=39914)\u001b[0m wandb: Tracking run with wandb version 0.19.6\n","\u001b[36m(train_with_tune pid=39914)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2025-02-13_13-01-03_983708_35281/artifacts/2025-02-13_13-01-06/train_with_tune_2025-02-13_13-01-01/working_dirs/train_with_tune_2f7ab0cc_2_batch_size=8,gradient_accumulation_steps=2,lora_alpha=128,lora_dropout=0.0194,lora_r=8,lr=0.0004,warmup_2025-02-13_13-01-16/wandb/run-20250213_131326-cc2eqhqs\n","\u001b[36m(train_with_tune pid=39914)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n","\u001b[36m(train_with_tune pid=39914)\u001b[0m wandb: Syncing run /content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_Deepseek-coder_Models/4_Try\n","\u001b[36m(train_with_tune pid=39914)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/fabianhensel-technische-universit-t-hamburg-harburg/huggingface\n","\u001b[36m(train_with_tune pid=39914)\u001b[0m wandb: 🚀 View run at https://wandb.ai/fabianhensel-technische-universit-t-hamburg-harburg/huggingface/runs/cc2eqhqs\n","  0%|          | 0/355 [00:00<?, ?it/s]\n","\u001b[36m(train_with_tune pid=39914)\u001b[0m The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n","  0%|          | 1/355 [00:02<14:03,  2.38s/it]\n","  1%|          | 2/355 [00:04<12:09,  2.07s/it]\n","  1%|          | 3/355 [00:06<11:34,  1.97s/it]\n","  1%|          | 4/355 [00:07<11:17,  1.93s/it]\n","  1%|▏         | 5/355 [00:09<11:07,  1.91s/it]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 1 TERMINATED | 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:13:38. Total running time: 12min 32s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status                lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout     iter     total time (s)     eval_loss     perplexity |\n","+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_2f7ab0cc   RUNNING      0.00035068               8               50        0.0855096                        2          8            128        0.0194159                                                          |\n","| train_with_tune_3fc94630   TERMINATED   4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948        1            715.985       1.03796        2.82346 |\n","| train_with_tune_fcc86c01   PENDING      2.69258e-05              4              150        0.0486618                        8         16             32        0.0892215                                                          |\n","+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 6/355 [00:11<11:00,  1.89s/it]\n","  2%|▏         | 7/355 [00:13<10:55,  1.88s/it]\n","  2%|▏         | 8/355 [00:15<10:52,  1.88s/it]\n","  3%|▎         | 9/355 [00:17<10:49,  1.88s/it]\n","  3%|▎         | 10/355 [00:19<10:47,  1.88s/it]\n","  3%|▎         | 11/355 [00:21<10:45,  1.88s/it]\n","  3%|▎         | 12/355 [00:22<10:43,  1.88s/it]\n","2025-02-13 13:13:52,537\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n","2025-02-13 13:13:52,603\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/ray_results/6_Try/train_with_tune_2025-02-13_13-01-01' in 0.0638s.\n","  4%|▎         | 13/355 [00:24<10:42,  1.88s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 TERMINATED | 1 RUNNING | 1 PENDING\n","Current time: 2025-02-13 13:13:52. Total running time: 12min 45s\n","Logical resource usage: 1.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:L4)\n","+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status                lr     batch_size     warmup_steps     weight_decay     ...ccumulation_steps     lora_r     lora_alpha     lora_dropout     iter     total time (s)     eval_loss     perplexity |\n","+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_with_tune_2f7ab0cc   RUNNING      0.00035068               8               50        0.0855096                        2          8            128        0.0194159                                                          |\n","| train_with_tune_3fc94630   TERMINATED   4.91278e-05              8               50        0.0108653                        4         32             64        0.0519948        1            715.985       1.03796        2.82346 |\n","| train_with_tune_fcc86c01   PENDING      2.69258e-05              4              150        0.0486618                        8         16             32        0.0892215                                                          |\n","+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 14/355 [00:26<10:40,  1.88s/it]\n","  4%|▍         | 15/355 [00:28<10:39,  1.88s/it]\n","  5%|▍         | 16/355 [00:30<10:37,  1.88s/it]\n","  5%|▍         | 17/355 [00:32<10:36,  1.88s/it]\n","  5%|▌         | 18/355 [00:34<10:34,  1.88s/it]\n","2025-02-13 13:14:02,615\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n","Resume experiment with: Tuner.restore(path=\"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/ray_results/6_Try/train_with_tune_2025-02-13_13-01-01\", trainable=...)\n","2025-02-13 13:14:02,635\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n","- train_with_tune_fcc86c01: FileNotFoundError('Could not fetch metrics for train_with_tune_fcc86c01: both result.json and progress.csv were not found at /content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/ray_results/6_Try/train_with_tune_2025-02-13_13-01-01/train_with_tune_fcc86c01_3_batch_size=4,gradient_accumulation_steps=8,lora_alpha=32,lora_dropout=0.0892,lora_r=16,lr=0.0000,warmup_2025-02-13_13-13-22')\n"]},{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["results = tuner.fit()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":949,"status":"ok","timestamp":1739452448942,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"ryfKwF3zL591","outputId":"6c5eafb9-e6ff-4bcc-e013-e3712cd36da2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Config: {'lr': 4.912780695509994e-05, 'batch_size': 8, 'warmup_steps': 50, 'weight_decay': 0.010865341218750567, 'gradient_accumulation_steps': 4, 'lora_r': 32, 'lora_alpha': 64, 'lora_dropout': 0.05199484805837334}\n"]}],"source":["print(\"Best Config:\", results.get_best_result(metric=\"eval_loss\", mode=\"min\").config)"]},{"cell_type":"markdown","metadata":{"id":"yEFmguVZQzeF"},"source":["# Training with SFTTrainer API"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGZ_TXMoJueI"},"outputs":[],"source":["from transformers import TrainingArguments\n","from trl import SFTConfig\n","\n","batch_size = 8\n","\n","# training_args = TrainingArguments(\n","#     # \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_CodeBERTa_Models/\",\n","#     # \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_CodeT5+_Models/\",\n","#     # \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_codegen-350M-mono/\",\n","#     # \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_Starcoder_Models/\",\n","#     # \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_CodeLlama_Models/1_Try\",\n","#     # \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_Starcoder_Models/2_Try\",\n","#     # \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_Starcoder_2_Models/8_Try\",\n","#     \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_Deepseek-coder_Models/8_Try\",\n","#     overwrite_output_dir=True,\n","\n","#     do_eval=True,\n","#     eval_strategy='steps',\n","#     eval_steps=283,\n","\n","#     learning_rate=0.00016,\n","\n","#     # Regularization technique to prevent overfitting\n","#     weight_decay=0.0534,\n","\n","#     per_device_train_batch_size=batch_size,\n","#     per_device_eval_batch_size=batch_size,\n","\n","#     # A memory-efficient variant of the AdamW optimizer\n","#     optim=\"paged_adamw_8bit\",\n","\n","#     logging_strategy=\"steps\",\n","#     logging_steps=100,\n","\n","#     # brain float 16, a special datatype for deep learning. (Is not supported by every GPU)\n","#     bf16=True,\n","\n","#     # Accumulates gradients over several batches and the optimizer is only active after a certain number of batches have been performed.\n","#     gradient_accumulation_steps=2,\n","\n","#     # Recomputes the intermediate values of a deep net (which would ordinarily be stored at forward time) at backward time. (saves memory during training)\n","#     gradient_checkpointing=True,\n","\n","#     # During warmup the learning rate is set to a very small value and increases linearly over the warmup steps until it reaches the base learning rate.\n","#     warmup_steps=100,\n","\n","#     # The maximal training steps\n","#     max_steps=2264,\n","# )\n","\n","sft_config = SFTConfig(\n","    # \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_CodeBERTa_Models/\",\n","    # \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_CodeT5+_Models/\",\n","    # \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_codegen-350M-mono/\",\n","    # \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_Starcoder_Models/\",\n","    # \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_CodeLlama_Models/1_Try\",\n","    # \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_Starcoder_Models/2_Try\",\n","    # \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_Starcoder_2_Models/8_Try\",\n","    \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_Deepseek-coder_Models/8_Try\",\n","    overwrite_output_dir=True,\n","\n","    do_eval=True,\n","    eval_strategy='steps',\n","    eval_steps=200,\n","\n","    learning_rate=0.00016,\n","\n","    # Regularization technique to prevent overfitting\n","    weight_decay=0.0534,\n","\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","\n","    # A memory-efficient variant of the AdamW optimizer\n","    optim=\"paged_adamw_8bit\",\n","\n","    logging_strategy=\"steps\",\n","    logging_steps=100,\n","\n","    # brain float 16, a special datatype for deep learning. (Is not supported by every GPU)\n","    bf16=True,\n","\n","    # Accumulates gradients over several batches and the optimizer is only active after a certain number of batches have been performed.\n","    gradient_accumulation_steps=2,\n","\n","    # Recomputes the intermediate values of a deep net (which would ordinarily be stored at forward time) at backward time. (saves memory during training)\n","    gradient_checkpointing=True,\n","\n","    # During warmup the learning rate is set to a very small value and increases linearly over the warmup steps until it reaches the base learning rate.\n","    warmup_steps=100,\n","\n","    # The maximal training steps\n","    # max_steps=1132,\n","    num_train_epochs=2,\n","\n","    max_seq_length=tokenizer.model_max_length,\n","\n","    # This will pack multiple short examples in the same input sequence. (inreases training efficiency, but has no impact in this case, as padding was set to true)\n","    # Unfortunately, packing negatively impacts the generation results when used with padding set to false.\n","    packing=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pkdpj8l6JueI"},"outputs":[],"source":["from trl import SFTTrainer\n","# from transformers import Trainer\n","import torch\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    args=sft_config,\n","    train_dataset=lm_dataset[\"train\"],\n","    eval_dataset=lm_dataset[\"validation\"],\n","    data_collator=FIMDataCollator(\n","        tokenizer=tokenizer,\n","        mlm=False\n","    ),\n","    tokenizer=tokenizer\n",")\n","\n","# trainer = Trainer(\n","#     model_init=model_init,\n","#     args=training_args,\n","#     train_dataset=lm_dataset[\"train\"],\n","#     eval_dataset=lm_dataset[\"validation\"],\n","#     data_collator=FIMDataCollator(\n","#         tokenizer=tokenizer,\n","#         mlm=False\n","#     ),\n","#     tokenizer=tokenizer\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":800},"executionInfo":{"elapsed":5021976,"status":"ok","timestamp":1739656412987,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"i_zTxNuGJueJ","outputId":"c99a17d8-7086-487b-a27e-ad370c7b0b01"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfabianhensel\u001b[0m (\u001b[33mfabianhensel-technische-universit-t-hamburg-harburg\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"data":{"text/html":["Tracking run with wandb version 0.19.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20250215_202954-euxeesnd</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/fabianhensel-technische-universit-t-hamburg-harburg/huggingface/runs/euxeesnd' target=\"_blank\">/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_Deepseek-coder_Models/8_Try</a></strong> to <a href='https://wandb.ai/fabianhensel-technische-universit-t-hamburg-harburg/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/fabianhensel-technische-universit-t-hamburg-harburg/huggingface' target=\"_blank\">https://wandb.ai/fabianhensel-technische-universit-t-hamburg-harburg/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/fabianhensel-technische-universit-t-hamburg-harburg/huggingface/runs/euxeesnd' target=\"_blank\">https://wandb.ai/fabianhensel-technische-universit-t-hamburg-harburg/huggingface/runs/euxeesnd</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2274' max='2274' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2274/2274 1:23:30, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>200</td>\n","      <td>1.012000</td>\n","      <td>0.957778</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.884200</td>\n","      <td>0.879291</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.860200</td>\n","      <td>0.838471</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.821500</td>\n","      <td>0.817711</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.812100</td>\n","      <td>0.800568</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.763800</td>\n","      <td>0.784666</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.769700</td>\n","      <td>0.780529</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.773100</td>\n","      <td>0.768817</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.748500</td>\n","      <td>0.762932</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.730600</td>\n","      <td>0.757444</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.698200</td>\n","      <td>0.753615</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n","  warnings.warn(\n"]},{"data":{"text/plain":["TrainOutput(global_step=2274, training_loss=0.8341370954152881, metrics={'train_runtime': 5021.4659, 'train_samples_per_second': 7.244, 'train_steps_per_second': 0.453, 'total_flos': 7.2921140070187e+16, 'train_loss': 0.8341370954152881, 'epoch': 2.0})"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4643,"status":"ok","timestamp":1739656424322,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"108lOmRJfiXj","outputId":"381e83b6-352c-4e66-f819-971c6c04b946"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n","  warnings.warn(\n"]}],"source":["trainer.save_model(\"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_Deepseek-coder_Models/8_Try/Model\")"]},{"cell_type":"markdown","metadata":{"id":"wr_YlkiLQ9Js"},"source":["# Evaluation with Perplexity, BLEU, and METEOR"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"elapsed":72384,"status":"ok","timestamp":1739656498588,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"V6aQZcurzkdD","outputId":"2007f2b8-5159-4e6d-bb2f-a2b6bbd4c651"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='251' max='251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [251/251 01:11]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Perplexity: 2.12 Validation Accuracy: None\n"]}],"source":["import math\n","\n","eval_results = trainer.evaluate()\n","print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\", f\"Validation Accuracy: {eval_results.get('eval_accuracy')}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r-A1GIzcTV9h"},"outputs":[],"source":["import torch\n","\n","# Clears GPU cache\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":52140,"status":"ok","timestamp":1743425680796,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-120},"id":"88SJuvoETbne","colab":{"base_uri":"https://localhost:8080/","height":249,"referenced_widgets":["320e9477e7114f249909436493539638","74a8754f14b142358885262b8cdc5bc0","c5a0f7a16ca847b4b9ea8012fa65f1ed","777d45b0e4fd4c9a9167e268e7fc0a67","4e905ddd1d21447ca6055e0e9ef3e8b7","bde07518e2954943874739011b6b27ce","00b92a748e80468dbb41733444730d76","819cc8cd03ea4d3c8b708f439acb89ad","77db752599404aeca04d58363e19bed1","2eb1cc9c646b440686f9a2d76e991e13","f5cd33d58f74460ca98b41eb6599d3d0","727cb4867b574a98967c864e28cbc365","b21e17b6c11e4a5490bc0d4cde490e9c","7906666ab7df4fad9d2649abec5d4fba","412cf5238a8644bfb3d00b44e0d014c6","e8414b81ea844878ba99e3fc1ce28828","d7ae49b5e78541a79bc515cc21c8888a","d6f0b8c8d61a469b9ac196a965134560","a7ed5f0d405244d9a651a37dd3128a7f","f8865d921e27473ba50dbe7d9dd287f7","2244aa3a69394c11b475c07690af0b8b","98ca5c7bf6e44d1d86528054dd84882d","13d889500ee94c0793f9f5ba544e5cdb","0bfc90d0e1cc445684dbd73ae5211a1e","a1f2c65c475a4572ae5ce63a5b56a712","96825b7cca28481c9120d9e58ffd279c","fa59a0703d6446e28a07f3d9be753aa9","c1752783543248adac396cd4d49948a7","ce78c7102c7f4216acff29af95cbb757","50be9c3eec6940a98600464d65837978","2a6420d4d14441859e414a7fd9d8c515","043a14e00a994b18b95aab99aad36cda","bc389d2e754e4fb18f6ba02baaa9f7b0","e8b63b12f2b541bab2fb37f1b25f6bd5","1513fbb726994f52a4c3651c666f95dc","9dce4dab36aa462a98c2c69953ae8ff1","35b394dfda084238b4c4af4a5d7c2537","1473604bfead4302acfd74637c5d6b7e","c7c10a1f0e814fb4a7813cfcb00ffa4b","b2cac655facc4047a1fd067b022554b7","09ce096301b944e0a9464694ac53b66b","07a6a98559404876af3e2129668f0569","fdce6ec644164e548344bfca57f3d43e","9ae2c405e2714e428cdbc9b6f771d929"]},"outputId":"8e4be989-7f01-485e-bf42-1fc6c86a2f37"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"320e9477e7114f249909436493539638"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/2.69G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"727cb4867b574a98967c864e28cbc365"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/2.69G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13d889500ee94c0793f9f5ba544e5cdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8b63b12f2b541bab2fb37f1b25f6bd5"}},"metadata":{}}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","from peft import PeftModel\n","import torch\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_checkpoint = 'deepseek-ai/deepseek-coder-1.3b-base'\n","\n","checkpoint = \"/content/drive/MyDrive/Colab_Notebooks/TUHH_Computer_Science_(Master)/Research_Project/Finetuned_Deepseek-coder_Models/8_Try/Model\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","old_model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n","old_model.resize_token_embeddings(len(tokenizer))\n","\n","finetuned_model = PeftModel.from_pretrained(old_model, checkpoint).to(device)"]},{"cell_type":"code","source":["finetuned_model.eval()"],"metadata":{"id":"ub9J-QVXGr6l"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3881,"status":"ok","timestamp":1743426842504,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-120},"id":"D7fzJWrIMZLY","outputId":"d0dd5bff-4c65-4c92-cb00-777fdf5daead"},"outputs":[{"output_type":"stream","name":"stdout","text":["\tfunction addLiquidityETH(\n","\t\taddress token,\n","\t\tuint256 amountTokenDesired,\n","\t\tuint256 amountTokenMin,\n","\t\tuint256 amountETHMin,\n","\t\taddress to,\n","\t\tuint256 deadline\n","\t) external payable returns (uint256 amountToken, uint256 amountETH, uint256 liquidity);\n"]}],"source":["text = '''<|secure_function|>\\tfunction add'''\n","model_inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","\n","input_ids = model_inputs[\"input_ids\"]\n","attention_mask = model_inputs[\"attention_mask\"]\n","\n","# eos_token = \"<|end▁of▁sentence|>\"\n","# eos_token_id = tokenizer.convert_tokens_to_ids(eos_token)\n","\n","generated_ids = finetuned_model.generate(input_ids,\n","                                         do_sample=True,\n","                                         max_length=256,\n","                                         num_beams=4,\n","                                         temperature=0.3,\n","                                         pad_token_id=tokenizer.eos_token_id,\n","                                         attention_mask=attention_mask)\n","\n","\n","# tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"hFCZxAVHFN5R","executionInfo":{"status":"ok","timestamp":1743426952743,"user_tz":-120,"elapsed":48,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"}}},"outputs":[],"source":["def generate_fim(prefix, suffix, model, tokenizer, max_length=256):\n","    input_text = f\"<|fim_begin|>{prefix}<|fim_hole|>{suffix}<|fim_end|>\"\n","    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n","    outputs = model.generate(\n","        inputs,\n","        max_length=max_length,\n","        num_beams=8,\n","        temperature=0.3,\n","        num_return_sequences=1,\n","        do_sample=True,\n","        pad_token_id=tokenizer.eos_token_id\n","    )\n","    middle = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n","    return prefix + middle + suffix"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3674,"status":"ok","timestamp":1743427026376,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-120},"id":"Ly0lVHJxFYcJ","outputId":"f6f9980d-7fc1-4065-9f37-91036b511cdf"},"outputs":[{"output_type":"stream","name":"stdout","text":["pragma solidity ^0.8.0;\n","\n","import \"@openzeppelin/contracts/utils/Context.sol\" as Context;\n","import \"@openzeppelin/contracts/interfaces/IERC20.sol\" as IERC20;\n","import \"@openzeppelin/contracts/access/Ownable.sol\" as Ownable;\n","\n","contract FOO is Context, IERC20, Ownable {\n"]}],"source":["prefix = '''pragma solidity ^0.8.0;\\n\\n'''\n","\n","suffix = '''\\n\\ncontract FOO is Context, IERC20, Ownable {'''\n","\n","print(generate_fim(prefix, suffix, finetuned_model, tokenizer))"]},{"cell_type":"code","source":["# Picks num_examples random FIM transformed constructs from the dataset and returns it\n","def return_random_FIMs(dataset, num_examples=10):\n","\n","    picks = []\n","    fim_set = []\n","    test_set = []\n","\n","    for i in range(len(dataset)):\n","        if '<|fim_end|>' in dataset[i]:\n","            fim_set.append(dataset[i])\n","\n","    for _ in range(num_examples):\n","        pick = random.randint(0, len(fim_set)-1)\n","        while pick in picks:\n","            pick = random.randint(0, len(fim_set)-1)\n","        picks.append(pick)\n","\n","    for pick in picks:\n","        prefix_index = fim_set[pick].index('<|fim_hole|>')\n","        suffix_index = fim_set[pick].index('<|fim_end|>')\n","\n","        prefix = fim_set[pick][0:prefix_index]\n","        ground_truth = fim_set[pick][suffix_index + len('<|fim_end|>'):len(fim_set[pick])-len('<|end_of_sentence|>')]\n","        suffix = fim_set[pick][prefix_index + len('<|fim_hole|>'):suffix_index]\n","\n","        test_set.append([prefix, suffix, ground_truth])\n","\n","    return test_set"],"metadata":{"id":"tnt0htCAERQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = return_random_FIMs(import_fim_dataset, 1)\n","\n","prompt = f\"{test[0][0]}<|fim_hole|>{test[0][1]}<|fim_end|>\"\n","reference = f\"{test[0][2]}\"\n","\n","generator = pipeline(\"text-generation\", model=finetuned_model, tokenizer=tokenizer, device=0)\n","print(generator(prompt, max_length=256, do_sample=True, num_beams=8, temperature=0.7, num_return_sequences=2, pad_token_id=tokenizer.eos_token_id)[0][\"generated_text\"])\n","print(reference, 1)"],"metadata":{"id":"CVBAsgUDH6oi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","import_fim_dataset = []\n","function_fim_dataset = []\n","vul_function_fim_dataset = []\n","constructor_fim_dataset = []\n","vul_constructor_fim_dataset = []\n","modifier_fim_dataset = []\n","\n","vul_pattern = r\".*\\/\\/ .*\"\n","\n","for i in range(len(updated_dataset['test'])):\n","    if 'pragma solidity ^0.8.0;' in updated_dataset['test'][i]['text']:\n","        import_fim_dataset.append(updated_dataset['test'][i]['text'])\n","\n","for i in range(len(updated_dataset['test'])):\n","    if '\\tfunction' in updated_dataset['test'][i]['text'] and not re.search(vul_pattern, updated_dataset['test'][i]['text']):\n","        function_fim_dataset.append(updated_dataset['test'][i]['text'])\n","\n","for i in range(len(updated_dataset['test'])):\n","    if '\\tfunction' in updated_dataset['test'][i]['text'] and re.search(vul_pattern, updated_dataset['test'][i]['text']):\n","        vul_function_fim_dataset.append(updated_dataset['test'][i]['text'])\n","\n","for i in range(len(updated_dataset['test'])):\n","    if '<|secure_constructor|>' in updated_dataset['test'][i]['text'] and not re.search(vul_pattern, updated_dataset['test'][i]['text']):\n","        constructor_fim_dataset.append(updated_dataset['test'][i]['text'])\n","\n","for i in range(len(updated_dataset['test'])):\n","    if 'constructor' in updated_dataset['test'][i]['text'] and re.search(vul_pattern, updated_dataset['test'][i]['text']):\n","        vul_constructor_fim_dataset.append(updated_dataset['test'][i]['text'])\n","\n","for i in range(len(updated_dataset['test'])):\n","    if '\\tmodifier' in updated_dataset['test'][i]['text']:\n","        modifier_fim_dataset.append(updated_dataset['test'][i]['text'])\n","\n","print(modifier_fim_dataset)"],"metadata":{"id":"A3rewqT3YHi2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pygments.lexers import SolidityLexer\n","from pygments.token import Token\n","from nltk.translate.meteor_score import meteor_score\n","import nltk\n","\n","nltk.download('wordnet')\n","\n","# Tokenizes the solidity code example\n","def tokenize_code(code):\n","    lexer = SolidityLexer()\n","    tokens = list(lexer.get_tokens(code))\n","    token_strings = []\n","    for token_type, token_value in tokens:\n","        if token_type not in (Token.Text, Token.Comment):\n","            token_strings.append(token_value)\n","    return token_strings\n","\n","# Stemming is not applied\n","class CodeStemmer:\n","    def stem(self, token):\n","        return token  # No stemming\n","\n","# Computes the METEOR score\n","def compute_meteor(generated_code, reference_code):\n","    score = 0\n","    for (gen_code, ref_code) in zip(generated_code, reference_code):\n","        gen_tokens = tokenize_code(gen_code)\n","        ref_tokens = tokenize_code(ref_code)\n","        score += meteor_score([ref_tokens], gen_tokens, preprocess=lambda x: x, stemmer=CodeStemmer())\n","\n","    return score / len(generated_code)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ql_LmEenqBsw","executionInfo":{"status":"ok","timestamp":1740339031628,"user_tz":-60,"elapsed":2575,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"}},"outputId":"7a683022-874f-4479-d939-69ca0426f301"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}]},{"cell_type":"code","source":["import evaluate\n","import re\n","\n","vul_pattern = r\".*\\/\\/ .*\"\n","\n","# Loads the BLEU metric\n","bleu = evaluate.load(\"bleu\")\n","\n","eos_token = \"<|end▁of▁sentence|>\"\n","eos_token_id = tokenizer.convert_tokens_to_ids(eos_token)\n","\n","# Generates predictions\n","def generate_code(model, tokenizer, prompts):\n","    inputs = []\n","    for prompt in prompts:\n","        inputs.append(tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device))\n","\n","    outputs = []\n","    for input in inputs:\n","        outputs.append(model.generate(input, max_length=256, num_beams=4, temperature=0.3, do_sample=True, pad_token_id=tokenizer.eos_token_id))\n","\n","    return [tokenizer.decode(output[0][len(input[0]):], skip_special_tokens=True) for (output, input) in zip(outputs, inputs)]\n","\n","# The prompts that should be completed\n","prompts = []\n","references = []\n","for prompt in return_random_FIMs(constructor_fim_dataset, 10):\n","    prompts.append(f\"{prompt[0]}<|fim_hole|>{prompt[1]}<|fim_end|>\")\n","    references.append(prompt[2])\n","\n","pretrained_predictions = generate_code(model, tokenizer, prompts)\n","finetuned_predictions = generate_code(finetuned_model, tokenizer, prompts)\n","\n","# Computes the BLEU score by comparing the predictions with the references\n","bleu_score_pretrained = 0\n","bleu_score_finetuned = 0\n","vulnerable_hits = 0\n","for (finetuned_prediction, pretrained_prediction, reference) in zip(finetuned_predictions, pretrained_predictions, references):\n","    if re.search(vul_pattern, finetuned_prediction):\n","        vulnerable_hits +=1                            # hit if vul_pattern is found in code fragment\n","    bleu_score_pretrained += bleu.compute(predictions=[pretrained_prediction], references=[reference])['bleu']\n","    bleu_score_finetuned += bleu.compute(predictions=[finetuned_prediction], references=[reference])['bleu']\n","\n","# The average of the scores is calculated\n","bleu_score_pretrained = bleu_score_pretrained / len(references)\n","bleu_score_finetuned = bleu_score_finetuned / len(references)\n","\n","# Computes the METEOR score by comparing the predictions with the references\n","meteor_score_pretrained = compute_meteor(pretrained_predictions, references)\n","meteor_score_finetuned = compute_meteor(finetuned_predictions, references)\n","\n","print(f\"Pretrained Model BLEU: {bleu_score_pretrained:.2f}\")\n","print(f\"Fine-Tuned Model BLEU: {bleu_score_finetuned:.2f}\")\n","print(f\"Pretrained Model METEOR: {meteor_score_pretrained:.2f}\")\n","print(f\"Fine-Tuned Model METEOR: {meteor_score_finetuned:.2f}\")\n","print(f\"Non-generated security comments: {vulnerable_hits}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tVr-3uLw8JjB","executionInfo":{"status":"ok","timestamp":1740071647444,"user_tz":-60,"elapsed":85449,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"}},"outputId":"458fce8a-a472-4dee-fdec-7d1f19e6a01a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pretrained Model BLEU: 0.01\n","Fine-Tuned Model BLEU: 0.46\n","Pretrained Model METEOR: 0.13\n","Fine-Tuned Model METEOR: 0.73\n","Non-generated security comments: 0\n"]}]},{"cell_type":"code","source":["import evaluate\n","\n","bleu = evaluate.load(\"bleu\")\n","\n","reference_code = ['\\n\\t\\t_;']\n","generated_code = ['\\n\\t\\tdf;']\n","\n","score = compute_meteor(generated_code, reference_code)\n","score_bleu = bleu.compute(predictions=generated_code, references=reference_code)\n","print(f\"METEOR Score: {score}\")\n","print(f\"BLEU Score: {score_bleu['bleu']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fjh8Qbf-pwWu","executionInfo":{"status":"ok","timestamp":1740339232010,"user_tz":-60,"elapsed":2158,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"}},"outputId":"96892eb7-e4d8-4008-cd39-8884fbfe52eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["METEOR Score: 0.9814814814814815\n","BLEU Score: 0.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2910,"status":"ok","timestamp":1740248157235,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"ZlzpsLPlwyTy","outputId":"9cf467db-972f-4591-b814-d81655a91810"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(32028, 2048)"]},"metadata":{},"execution_count":13}],"source":["from transformers import AutoModelForCausalLM\n","\n","device = \"cuda\"\n","\n","# Base model\n","model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)\n","\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124,"referenced_widgets":["085884a54f824f56ac3e3e07bec700d9","42cf6ce06fdd409b9fa032636b40e6eb","1f344a4c41c049a49d45db6cd2d41b4e","1526eaebcb6848fd839450865e97e814","f41da428a41d4b5a95ae2dd826b974d1","f513fa45b1bc472f85cff56540e6dd89","244533634e1543188e6347846e24dd18","fe26371d027c44a69147878604b4e86c","9962238662c34455b3112b841e213931","6749cfb1abf24df6ac387c969722678a","2105977bad734ff0801c6c3e9dce3e4c"]},"executionInfo":{"elapsed":160758,"status":"ok","timestamp":1739714382833,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"L7ZYUNnVwv87","outputId":"dbf8d47f-837e-4dd7-ca33-8088e1a57117"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-16-e722d5c5bcc6>:14: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  trainer = SFTTrainer(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"085884a54f824f56ac3e3e07bec700d9","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='251' max='251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [251/251 02:37]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Perplexity: 12.08\n"]}],"source":["from trl import SFTConfig, SFTTrainer\n","import math\n","\n","sft_config = SFTConfig(\n","    output_dir=\"./results\",\n","    save_strategy=\"no\",\n","    per_device_eval_batch_size=8,\n","    logging_dir=\"./logs\",\n","    report_to=\"none\",\n","    packing=True,\n","    max_seq_length=tokenizer.model_max_length,\n",")\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    args=sft_config,\n","    eval_dataset=lm_dataset[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=FIMDataCollator(\n","        tokenizer=tokenizer,\n","        mlm=False\n","    )\n",")\n","\n","eval_results = trainer.evaluate()\n","perplexity = math.exp(eval_results[\"eval_loss\"])\n","\n","print(f\"Perplexity: {perplexity:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124,"referenced_widgets":["5ac46b8d47964f96a8e4075ef1edacd0","e57045178adf4e95b1a044e03b29740f","6bccb927bbc04527991f04c52a04e6a3","112622c3f68245f99ee9ddc55106d2dd","d8f5107c74794866ab86e73256df8ed6","db689e025417426c83f42bf43049b072","5823cba49784417ba25898d778d7736e","3eaaacd2d6be46d7a917b65d290e2e2a","0e6ef6142500414ead8953072c15730a","b1e4083f86e5445c9ba9e0c8acc2d2a2","960d3950f31846cdbca1b7f6fd50257c"]},"executionInfo":{"elapsed":172013,"status":"ok","timestamp":1739714152726,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"VFsYfTAzuFWi","outputId":"668cb56c-40e9-4fdd-f522-be7b6d454cf1"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-14-cd8a8d28ec9e>:14: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  trainer = SFTTrainer(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5ac46b8d47964f96a8e4075ef1edacd0","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='251' max='251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [251/251 02:46]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Perplexity: 2.19\n"]}],"source":["from trl import SFTConfig, SFTTrainer\n","import math\n","\n","sft_config = SFTConfig(\n","    output_dir=\"./results\",\n","    save_strategy=\"no\",\n","    per_device_eval_batch_size=8,\n","    logging_dir=\"./logs\",\n","    report_to=\"none\",\n","    packing=True,\n","    max_seq_length=tokenizer.model_max_length,\n",")\n","\n","trainer = SFTTrainer(\n","    model=finetuned_model,\n","    args=sft_config,\n","    eval_dataset=lm_dataset[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=FIMDataCollator(\n","        tokenizer=tokenizer,\n","        mlm=False\n","    )\n",")\n","\n","eval_results = trainer.evaluate()\n","perplexity = math.exp(eval_results[\"eval_loss\"])\n","\n","print(f\"Perplexity: {perplexity:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":194123,"status":"ok","timestamp":1740249121871,"user":{"displayName":"Fabian Hensel","userId":"03825662142625868714"},"user_tz":-60},"id":"SyEjYaP62sRX","outputId":"e2e011b4-11a9-4d1b-9159-e59fd9a6cfcc"},"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n","Device set to use cuda:0\n","The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"]},{"output_type":"stream","name":"stdout","text":["Pretrained Model BLEU: 0.11\n","Fine-Tuned Model BLEU: 0.55\n","Pretrained Model METEOR: 0.30\n","Fine-Tuned Model METEOR: 0.90\n","Vulnerable Hits: 0\n"]}],"source":["from transformers import pipeline\n","import evaluate\n","import re\n","\n","vul_pattern = r\".*\\/\\/ .*\"\n","\n","# Loads the BLEU metric\n","bleu = evaluate.load(\"bleu\")\n","\n","eos_token = \"<|end▁of▁sentence|>\"\n","eos_token_id = tokenizer.convert_tokens_to_ids(eos_token)\n","\n","# Generates predictions\n","def generate_code(model, tokenizer, prompts):\n","    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n","    return [generator(prompt, max_length=256, do_sample=True, num_beams=4, temperature=0.3, pad_token_id=tokenizer.eos_token_id)[0][\"generated_text\"] for prompt in prompts]\n","\n","# The prompts that should be completed\n","prompts = ['<|secure_function|>\\tfunction _transfer(\\n\\t\\taddress sender,\\n\\t\\taddress recipient,\\n\\t\\tuint256 amount\\n\\t) internal virtual {',\n","           '<|secure_function|>\\tfunction _approve(\\n\\t\\taddress owner,\\n\\t\\taddress spender,\\n\\t\\tuint256 amount\\n\\t) internal virtual {',\n","\t\t\t\t\t '<|secure_function|>\\tfunction approve(\\n\\t\\taddress spender,\\n\\t\\tuint256 amount\\n\\t) public returns (bool success) {',\n","           '<|secure_function|>\\tfunction transfer(\\n\\t\\taddress from,\\n\\t\\taddress to,\\n\\t\\tuint256 amount\\n\\t) public virtual override returns (bool) {',\n","\t\t\t\t\t '<|secure_function|>\\tfunction withdraw(',\n","\t\t\t\t\t '<|secure_function|>\\tfunction add(uint256 a, uint256 b) internal pure returns (uint256) {',\n","\t\t\t\t\t '<|secure_function|>\\tfunction sub(uint256 a, uint256 b) internal pure returns (uint256) {',\n","\t\t\t\t\t '<|secure_function|>\\tfunction div(uint256 a, uint256 b) internal pure returns (uint256) {',\n","           '<|secure_function|>\\tfunction mult(uint256 a, uint256 b) internal pure returns (uint256) {',\n","           '<|secure_function|>\\tfunction sendValue(address payable recipient, uint256 amount) internal {',\n","           '<|secure_function|>\\tfunction ownerOf(\\n\\t\\tuint256 tokenId\\n\\t) public view virtual override returns (address owner) {',\n","           '<|secure_function|>\\tfunction symbol',\n","           '<|secure_function|>\\tfunction name',\n","           '\\tmodifer onlyOwner() {',\n","           '\\tevent Approval',\n","           '\\tevent Transfer',\n","           '\\tusing SafeMath',\n","           '\\tusing Address',\n","           '<|secure_function|>\\tfunction burn',\n","           '\\tstruct CurrentRateInfo {'\n","]\n","\n","pretrained_predictions = generate_code(model, tokenizer, prompts)\n","finetuned_predictions = generate_code(finetuned_model, tokenizer, prompts)\n","\n","# The references that serve as ground truth\n","references = ['\\n\\t\\trequire(sender != address(0), \"ERC20: transfer from the zero address\");\\n\\n\\t\\trequire(recipient != address(0), \"ERC20: transfer to the zero address\");\\n\\n\\t\\tuint256 senderBalance = _balances[sender];\\n\\n\\t\\trequire(\\n\\t\\t\\tsenderBalance >= amount,\\n\\t\\t\\t\"ERC20: transfer amount exceeds balance\"\\n\\t\\t);\\n\\n\\t\\tunchecked {\\n\\t\\t\\t_balances[sender] = senderBalance - amount;\\n\\n\\t\\t\\t_balances[recipient] += amount;\\n\\t\\t}\\n\\n\\t\\temit Transfer(sender, recipient, amount);\\n\\t}',\n","              '\\n\\t\\trequire(owner != address(0), \"ERC20: approve from the zero address\");\\n\\n\\t\\trequire(spender != address(0), \"ERC20: approve to the zero address\");\\n\\n\\t\\t_allowances[owner][spender] = amount;\\n\\n\\t\\temit Approval(owner, spender, amount);\\n\\t}',\n","              '\\n\\t\\tallowances[msg.sender][spender] = amount;\\n\\t\\temit Approval(msg.sender, spender, amount);\\n\\t\\treturn true;\\n\\t}',\n","              '\\n\\t\\trequire(from != address(0), \"ERC20: transfer from the zero address\");\\n\\n\\t\\trequire(to != address(0), \"ERC20: transfer to the zero address\");\\n\\n\\t\\tuint256 fromBalance = _balances[from];\\n\\n\\t\\trequire(\\n\\t\\t\\tfromBalance >= amount,\\n\\t\\t\\t\"ERC20: transfer amount exceeds balance\"\\n\\t\\t);\\n\\n\\t\\tunchecked {\\n\\t\\t\\t_balances[from] = fromBalance - amount;\\n\\n\\t\\t\\t_balances[to] += amount;\\n\\t\\t}\\n\\n\\t\\temit Transfer(from, to, amount);\\n\\n\\t\\treturn true;\\n\\t}',\n","              'uint256 amount) external onlyOwner {\\n\\t\\tpayable(msg.sender).transfer(amount);\\n\\t}',\n","              '\\n\\t\\tunchecked {\\n\\t\\t\\tuint256 c = a + b;\\n\\n\\t\\t\\trequire(c >= a, \"SafeMath: addition overflow\");\\n\\n\\t\\t\\treturn c;\\n\\t\\t}\\n\\t}',\n","              '\\n\\t\\treturn sub(a, b, \"SafeMath: subtraction overflow\");\\n\\t}',\n","              '\\n\\t\\treturn div(a, b, \"SafeMath: division by zero\");\\n\\t}',\n","              '\\n\\t\\treturn a * b;\\n\\t}',\n","              '\\n\\t\\t(bool success, ) = recipient.call{value: amount}(\"\");\\n\\t\\trequire(success, \"Address: unable to send value, recipient may have reverted\");\\n\\t}',\n","              '\\n\\t\\trequire(_exists(tokenId), \"ERC721: owner of nonexistent token\");\\n\\n\\t\\treturn _owners[tokenId];\\n\\t}',\n","              '() public view virtual override returns (string memory) {\\n\\t\\treturn _symbol;\\n\\t}',\n","              '() public view virtual override returns (string memory) {\\n\\t\\treturn _name;\\n\\t}',\n","              '\\n\\t\\t_transferOwnership(address(0));\\n\\t}',\n","              '(\\n\\t\\taddress indexed owner,\\n\\t\\taddress indexed spender,\\n\\t\\tuint256 value\\n\\t);',\n","              '(\\n\\t\\taddress indexed from,\\n\\t\\taddress indexed to,\\n\\t\\tuint256 indexed id\\n\\t);',\n","              ' for uint256;',\n","              ' for address;',\n","              '(uint256 amount) external onlyOwner {\\n\\t\\t_burn(msg.sender, amount);\\n\\t}',\n","              '\\n\\t\\tuint64 lastTimestamp;\\n\\t\\tuint64 ratePerSec;\\n\\t\\tuint64 fullUtilizationRate;\\n\\t}'\n","]\n","\n","bleu_score_pretrained = 0\n","bleu_score_finetuned = 0\n","vulnerable_hits = 0\n","for (finetuned_prediction, pretrained_prediction, reference) in zip(finetuned_predictions, pretrained_predictions, references):\n","    if re.search(vul_pattern, finetuned_prediction):\n","        vulnerable_hits +=1\n","    bleu_score_pretrained += bleu.compute(predictions=[pretrained_prediction], references=[reference])['bleu']\n","    bleu_score_finetuned += bleu.compute(predictions=[finetuned_prediction], references=[reference])['bleu']\n","\n","bleu_score_pretrained = bleu_score_pretrained / len(references)\n","bleu_score_finetuned = bleu_score_finetuned / len(references)\n","\n","meteor_score_pretrained = compute_meteor(pretrained_predictions, references)\n","meteor_score_finetuned = compute_meteor(finetuned_predictions, references)\n","\n","print(f\"Pretrained Model BLEU: {bleu_score_pretrained:.2f}\")\n","print(f\"Fine-Tuned Model BLEU: {bleu_score_finetuned:.2f}\")\n","print(f\"Pretrained Model METEOR: {meteor_score_pretrained:.2f}\")\n","print(f\"Fine-Tuned Model METEOR: {meteor_score_finetuned:.2f}\")\n","print(f\"Vulnerable Hits: {vulnerable_hits}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"widgets":{}},"nbformat":4,"nbformat_minor":0}